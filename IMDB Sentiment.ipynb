{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IMDB Sentiment.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "bQpja1DZ-9Ph",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis on IMDb Reviews"
      ]
    },
    {
      "metadata": {
        "id": "0FPVcrx9-9Pm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function, division"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VPWLzHje-9Ps",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Download and extract the IMDB sentiment dataset:"
      ]
    },
    {
      "metadata": {
        "id": "abO-VAHX_hF3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "08e2bc10-5b80-4be3-f63c-ec815506d39d"
      },
      "cell_type": "code",
      "source": [
        "!    wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!    tar xfz aclImdb_v1.tar.gz"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-02-21 22:45:57--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘aclImdb_v1.tar.gz’\n",
            "\n",
            "aclImdb_v1.tar.gz   100%[===================>]  80.23M  23.3MB/s    in 4.1s    \n",
            "\n",
            "2019-02-21 22:46:07 (19.5 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZQXlpBLB-9Pv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "First, we pre-process the dataset, converting each review to a sequence of word index numbers."
      ]
    },
    {
      "metadata": {
        "id": "T3CrPOmM-9Pw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a8d303fa-bb8b-4069-dadf-b600c5ee95a5"
      },
      "cell_type": "code",
      "source": [
        "# Vocabulary: All words used, starting by the most frequent\n",
        "with open('aclImdb/imdb.vocab') as f:\n",
        "    vocab = [word.rstrip() for word in f]\n",
        "    # Keep only most frequent 5000 words rather than all 90000\n",
        "    # Just saving memory - the long tail occurs too few times\n",
        "    # for the model to learn anything anyway\n",
        "    vocab = vocab[:5000]\n",
        "    print('%d words in vocabulary' % (len(vocab),))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5000 words in vocabulary\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dh3XRMRf-9P3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def text_tokens(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(\"\\\\s\", \" \", text)\n",
        "    text = re.sub(\"[^a-zA-Z' ]\", \"\", text)\n",
        "    tokens = text.split(' ')\n",
        "    return tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZDwrjCef-9P7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def load_dataset(dirname):\n",
        "    X, y = [], []\n",
        "    # Review files: neg/0_3.txt neg/10000_4.txt neg/10001_4.txt ...\n",
        "    for y_val, y_label in enumerate(['neg', 'pos']):\n",
        "        y_dir = os.path.join(dirname, y_label)\n",
        "        for fname in os.listdir(y_dir):\n",
        "            fpath = os.path.join(y_dir, fname)\n",
        "            # print('\\r' + fpath + '   ', end='')\n",
        "            with open(fpath) as f:\n",
        "                tokens = text_tokens(f.read())\n",
        "            X.append(tokens)\n",
        "            y.append(y_val)  # 0 for 'neg', 1 for 'pos'\n",
        "    print()\n",
        "    return X, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "Dk5yi6O7-9P_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "750dc914-e552-4a35-904a-cd49261f1b51"
      },
      "cell_type": "code",
      "source": [
        "X_train, y_train = load_dataset('aclImdb/train/')\n",
        "\n",
        "# We are cheating here - this is a test set, not a validation set.\n",
        "# This is just to make results quickly comparable to outside results\n",
        "# during the tutorial, but you should normally never use the test set\n",
        "# during training, of course!\n",
        "X_val, y_val = load_dataset('aclImdb/test/')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "i0Kyi8xV-9QE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3157bc7e-888f-4595-a48b-a199605d469b"
      },
      "cell_type": "code",
      "source": [
        "len(X_train), len(X_val)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 25000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "QoLumL04-9QK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Bag-of-words Linear Model"
      ]
    },
    {
      "metadata": {
        "id": "KOQo5OP_-9QL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def bow_onehot_vector(tokens):\n",
        "    vector = [0] * len(vocab)\n",
        "    for t in tokens:\n",
        "        try:\n",
        "            vector[vocab.index(t)] = 1\n",
        "        except:\n",
        "            pass  # ignore missing words\n",
        "    return vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qpSiUfNh-9QP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9ac4cb58-6e45-44c7-beee-019074ca2394"
      },
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "X_bow_train = [bow_onehot_vector(x) for x in tqdm(X_train)]\n",
        "X_bow_val = [bow_onehot_vector(x) for x in tqdm(X_val)]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 25000/25000 [01:27<00:00, 285.03it/s]\n",
            "100%|██████████| 25000/25000 [01:25<00:00, 291.52it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "pxZY20nQ-9QU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def best_train_history(history):\n",
        "    best_epoch = np.argmax(history.history['val_acc'])\n",
        "    print('Accuracy (epoch %d): %.4f train, %.4f val' % \\\n",
        "          (best_epoch + 1, history.history['acc'][best_epoch], history.history['val_acc'][best_epoch]))\n",
        "# (Note that sentiment.model is the state after the last epoch rather than best epoch!\n",
        "# Use ModelCheckpointer to restore the best epoch.)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "slkvuH-s-9Qd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        },
        "outputId": "ce2fc83e-e158-4026-8337-e26494392622"
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Activation, Dense, Input\n",
        "from keras.models import Model\n",
        "import numpy as np\n",
        "\n",
        "class BOWSentimentModel(object):\n",
        "    def __init__(self):\n",
        "        bow = Input(shape=(len(vocab),), name='bow_input')\n",
        "        # weights of all inputs\n",
        "        sentiment = Dense(1)(bow)\n",
        "        # normalize to [0, 1] range\n",
        "        sentiment = Activation('sigmoid')(sentiment)\n",
        "\n",
        "        self.model = Model(inputs=[bow], outputs=[sentiment])\n",
        "        self.model.summary()\n",
        "        self.model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    def train(self, X, y, X_val, y_val):\n",
        "        print('Fitting...')\n",
        "        return self.model.fit(np.array(X), y, validation_data=(np.array(X_val), y_val), epochs=10, verbose=1)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.model.predict(np.array(X))\n",
        "    \n",
        "sentiment = BOWSentimentModel()\n",
        "history = sentiment.train(X_bow_train, y_train, X_bow_val, y_val)\n",
        "best_train_history(history)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bow_input (InputLayer)       (None, 5000)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 5001      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 5,001\n",
            "Trainable params: 5,001\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Fitting...\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/10\n",
            "25000/25000 [==============================] - 3s 112us/step - loss: 0.4523 - acc: 0.8350 - val_loss: 0.3628 - val_acc: 0.8719\n",
            "Epoch 2/10\n",
            "25000/25000 [==============================] - 2s 86us/step - loss: 0.3136 - acc: 0.8877 - val_loss: 0.3165 - val_acc: 0.8792\n",
            "Epoch 3/10\n",
            "25000/25000 [==============================] - 2s 86us/step - loss: 0.2729 - acc: 0.9016 - val_loss: 0.3005 - val_acc: 0.8804\n",
            "Epoch 4/10\n",
            "25000/25000 [==============================] - 2s 87us/step - loss: 0.2502 - acc: 0.9081 - val_loss: 0.2936 - val_acc: 0.8815\n",
            "Epoch 5/10\n",
            "25000/25000 [==============================] - 2s 85us/step - loss: 0.2348 - acc: 0.9142 - val_loss: 0.2915 - val_acc: 0.8813\n",
            "Epoch 6/10\n",
            "25000/25000 [==============================] - 2s 85us/step - loss: 0.2234 - acc: 0.9182 - val_loss: 0.2921 - val_acc: 0.8805\n",
            "Epoch 7/10\n",
            "25000/25000 [==============================] - 2s 85us/step - loss: 0.2142 - acc: 0.9228 - val_loss: 0.2923 - val_acc: 0.8812\n",
            "Epoch 8/10\n",
            "25000/25000 [==============================] - 2s 85us/step - loss: 0.2069 - acc: 0.9239 - val_loss: 0.2947 - val_acc: 0.8794\n",
            "Epoch 9/10\n",
            "25000/25000 [==============================] - 2s 84us/step - loss: 0.2008 - acc: 0.9262 - val_loss: 0.2985 - val_acc: 0.8779\n",
            "Epoch 10/10\n",
            "25000/25000 [==============================] - 2s 86us/step - loss: 0.1953 - acc: 0.9297 - val_loss: 0.3006 - val_acc: 0.8772\n",
            "Accuracy (epoch 4): 0.9081 train, 0.8815 val\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CTe-Qt2N-9Qm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "0a2b9696-1c1c-47b3-b2e6-7859f5bc8579"
      },
      "cell_type": "code",
      "source": [
        "test_text = 'Good story about a backwoods community in the Ozarks around the turn of the century. Moonshine is the leading industry, fighting and funning the major form of entertainment. One day a stranger enters the community and causes a shake-up among the locals. Beautiful scenery adds much to the story.'\n",
        "test_tokens = text_tokens(test_text)\n",
        "print(test_text)\n",
        "print(sentiment.predict([bow_onehot_vector(test_tokens)])[0])\n",
        "\n",
        "test_text = 'Boring story about a backwoods community in the Ozarks around the turn of the century. Moonshine is the leading industry, fighting and funning the major form of entertainment. One day a stranger enters the community and causes a shake-up among the locals. Beautiful scenery does not add to the story.'\n",
        "test_tokens = text_tokens(test_text)\n",
        "print(test_text)\n",
        "print(sentiment.predict([bow_onehot_vector(test_tokens)])[0])\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Good story about a backwoods community in the Ozarks around the turn of the century. Moonshine is the leading industry, fighting and funning the major form of entertainment. One day a stranger enters the community and causes a shake-up among the locals. Beautiful scenery adds much to the story.\n",
            "[0.7635506]\n",
            "Boring story about a backwoods community in the Ozarks around the turn of the century. Moonshine is the leading industry, fighting and funning the major form of entertainment. One day a stranger enters the community and causes a shake-up among the locals. Beautiful scenery does not add to the story.\n",
            "[0.3953569]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1WBNFZb8-9Q2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot_train_history(history):\n",
        "    plt.figure()\n",
        "    plt.plot(history.history['acc'])\n",
        "    plt.plot(history.history['val_acc'])\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.legend(['acc', 'val_acc'])\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eLjC737--9Q6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "outputId": "1a03611b-f8ff-48d7-f512-a4128167c539"
      },
      "cell_type": "code",
      "source": [
        "plot_train_history(history)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAFYCAYAAABKymUhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3Xt81NWdP/7XZ+73ZCaZBBJCCAFB\nEhKgES8giA12Le121QJp1ytqbav7q67dR5fYLW2ptNi12tWuxYf6reuCxiq2dl1vtKCgCC5IAsEg\nGSCBBJJMMplkMveZz++PmQwJkDDAzHwmk9fz8eCRmc9c8s4B8ppzPp9zjiCKoggiIiIa82RSF0BE\nRESJwVAnIiLKEAx1IiKiDMFQJyIiyhAMdSIiogzBUCciIsoQCqkLuFRdXf0JfT+zWQeHw53Q96Sz\nsZ1Tg+2cOmzr1GA7A1arccTH2FM/g0Ihl7qEcYHtnBps59RhW6cG23l0DHUiIqIMwVAnIiLKEAx1\nIiKiDMFQJyIiyhAMdSIiogzBUCciIsoQDHUiIqIMwVAnIiLKEAx1IiKiDDHml4lNRwMDLvzsZz+G\nx+OB1+vFQw/9CwYGXNiw4T8hk8lQXX0DVqz4Nj799JOzjhEREV2sjA/1V//WjE+bOuN+vlwuIBQS\nR33OFTPzsOL6aSM+3t3dja997R+waNF12LPnU2zc+CJstmY888wLMJlMWL36YXzjGzfj8cfXn3VM\nrdbEXSsREaWnsCii3T6Aoyf7UJxvxOT8kddrT6SMD3UpWCw5ePHF5/Dyyy8hEAjA6/VApVLBbDYD\nAB577Ek4HD1nHSMiorHJ7Q3iyEknmk84YWvvw5H2Pnh8QQDA7Kk5eGhFZUrqyPhQX3H9tFF71Wey\nWo2XvPPbq69uQm5uHv7t39aiqekg1q37GcLh4b1/mUx21jEiIkp/oijiVI8bzW1O2Nr6YGtzot0+\ngKG/0fPNWsydnovSwix86TJrymrL+FCXgtPZi9LS6QCADz7YCp1Oj74+J7q6OpGba8WPfvQQ/u3f\n1iIcDp11zGhMzRANERHFx+sP4mh7XyTE2yMhPuANxh5XKWW4rCgb0yZlobQgC1MLTTDpVJLUylBP\ngr/7u2X4xS/WYOvWLbjllhXYsuU93HHHXfjxj38EALj++moYjUY8/PC/nnWMiIikI4oiOns9sEV7\n4c1tTpzockEc0g3PzdJg9tQclBZmYVphFibl6SGXpcdkMkEUxTE9BnypQ+VnSsTwO50f2zk12M6p\nw7ZOjUS3sy8QwrGTfbC190XPhzvR7w7EHlfIZZgy0YhphZFe+LRCE7IM6oR9/4thtY7cAWRPnYiI\nxgVRFNHd5x12Lvx4pwuhIdc3WUxqXDEzL9YLn5xvgEKeHr3weDDUiYgoIwWCIbScckVD3Inmdiec\nLn/scblMQPGEaC+8MAulBSZYTGN7WjFDnYiIMoKj3xcLcFubEy0d/QgOWXckS6/Cly6zxnrhxRMM\nUCrkElaceAx1IiIac4KhMFo7XJEAb3eiuc2Jnj5f7HGZIKAo3xDthZswrSALOVkaCIIgYdXJx1An\nIqK05guE0Onw4FSPGx2ftGJ/cxeOnepHIBiOPceoU2LOtNxIgBdmYcpEE9TKzOqFx4OhTkREkguG\nwuh2eiPB3ePGKYcHHT1udDjcw3rgACAIwCTr6V54aWEW8rK1Gd8LjwdDnYiIUiIsiujt9w0L7cEQ\ntzu9w65CH2Q2qnF5sRn5Fh3yzVrMnp4Hs04BrZrxdS5sFQl985tfx3/9Vx10Op3UpRARJYQoinB5\nAujoiQ6XO6I97x4POh1u+IcMmQ8yaJWYMtGICWYd8i06TLDokGfWIt+sg1o1fAid6wGMjqFOREQX\nzOsPoqPHgw7H6d72YHAPXUJ1kFopx4QcHfJjwa2N9r51MGiVEvwEmSnjQ31z8//gs879cT9fLhPO\nOQQ01Ny82bh52tdGfHzVqn/EunWPY8KECTh16iRWr34YVmvesP3VZ80qP28tL7/839i27a8Ih8O4\n+uoFWLXqO+jv78fPf/5jDAwMwGAw4Kc/XYdQKHTWMfb+iehSBYJhdPV6or1tz5Dz3e5h870HyWUC\n8sxaTJ+UHeltW7Sx3ne2QcVz3imQ8aEuhUWLluCjjz7ELbeswPbtH2DRoiUoLZ0+bH/1Rx/9dVzv\n9Z//+RxkMhlWrPgGVq78Nl5++SXMn381li+vQV3dRvzf/+1GU9PBs44tWnRdcn9IIsoI4bCInj4v\nTkWDezC0B89zn7mQuADAYtKgbEr0PHd0uDzfrEVOliZt1kAfrzI+1G+e9rVRe9VnSsT5mkWLluDp\np5/ELbeswI4dH+CBBx7CK6+8FNtfXaOJb8UijUaDBx74DuRyOXp7e9HX14cvvmjCPfd8DwCwcuU/\nAgDefHPzWceIaHwKhcNwe4Nwe4NweQNwe4MY8AQw4A1iwBvAgCcItzcAlycAu9OLDocHwdDZ57lN\nehWmF2YhLxbakSHzPLM24xZsySQZH+pSmDq1FN3dXejoOIX+/n5s375t2P7qTz/95Hnf49Spk6ir\n24gXXtgInU6H225bAQCQyeQQxeH/Ac91jIjGLlEU4QuEIsHsiQaz9xzBHA3soY97fGefzx6JVi3H\nJKs+EtrR3vbgeW6dhvEwFvFvLUmuvnohnn32P3HttYvR2+sYtr96MHj+/3S9vb0wm83Q6XQ4dKgJ\np06dQiAQwOWXz8KePZ/i8svL8Kc/vQ61Wn3OYzfeGP/oBBElRygcjgTx0OD1nB3QA94zgtsTOO+1\nPUOplXLoNArkmDTQaxTQa5XQaRQwaJTQaxXQaZSR49H7+uh9rVrB89wZhqGeJIsXL8F3v7sKf/jD\ny/B6PWftr/7WW2+O+vrp0y+DVqvD9763CrNnz8E3vnEzHn98PR599DH84hc/wQMPfAc6nR4//ekv\nEA6LZx0jouTz+UM42NKDg8cc8ARCcDi9w4a6vf5Q3O8lEwToooGcm6U5HcDqM4JZOySgNZHjSgXP\nY1ME91M/A+dApgbbOTXYzoln7/Wg3taNepsdTS29Z52P1qjksdDVnRnCQ3rQujN6zhqVnL3mOPDf\nNPdTT2s7dnyAV17ZeNbx5cu/hcWLl0hQERENFQqHYWvrQ73NjobmbrTZB2KPTbIaUDktBxWlOZg1\nLQ+eAe+Y2nubMg9DXWILFy7GwoWLpS6DiIZweQI4cKQbDbZu7D/SHVtMRamQoaI0B5XTclExNQc5\nWadnsmQb1Qh4z567TZRKDHUiGvdEUUSbfQANtm7UN9vR3OaMzc+2mNSYf3k+KkpzMLPYPC53/qKx\ng6FORONSIBjC5y29aLDZUd/cje4+L4DIDmClBVmonJaDytJcFFr1PNdNY0ZSQ33dunWor6+HIAio\nra1FRUVF7LEtW7bgmWeegUqlwrJly3DrrbcCAB577DHs2bMHwWAQ9913H2644YZklkhE44ij3xc7\nN36wpQf+QOQiN51agfmX56GyNBflUy0w6lQSV0p0cZIW6rt370ZLSwvq6upgs9lQW1uLuro6AEA4\nHMbatWvxxhtvIDs7G/feey+qq6tx7NgxHD58GHV1dXA4HLjpppsY6kR00cJhEUdP9qHe1o2GZjta\nO12xxybm6FA5LReVpTkoLcziBW6UEZIW6jt37kR1dTUAoLS0FE6nEy6XCwaDAQ6HAyaTCRaLBQBw\n1VVX4eOPP8Y3vvGNWG/eZDLB4/EgFApBLuc5LCKKj9sbROOxHjQ029FwpBv97gAAQCEXUFZiQWVp\nDiqm5SIvWytxpUSJl7RQt9vtKCsri923WCzo6uqCwWCAxWLBwMAAjh07hsLCQuzatQvz58+HXC6P\n7S722muvYdGiRecNdLNZB0WC1yEebQ4gJQ7bOTXGQzu3dbnw6cFT+PRgBxqPdMdWY7OY1LjhymJU\nXZ6POZdZoVUn9zKi8dDW6YDtPLKUXSg3dI0bQRDwq1/9CrW1tTAajZg0adKw527ZsgWvvfYaXnjh\nhfO+r8PhTmidXNggNdjOqZGp7RwMhXHoeC8amiOLwHQ6PLHHSiYaUVmai8ppuSjKN0AWvcjN1eeB\na6Q3TIBMbet0w3aWaPGZvLw82O322P3Ozk5YrdbY/fnz52PTpk0AgMcffxyFhYUAgO3bt+P3v/89\nnnvuORiN/DRGRBHOAT8aohe5NR7riS3BqlbJ8aUZVlSU5qBiag6yDGqJKyWSTtJCfcGCBXjqqadQ\nU1ODxsZG5OXlwWAwxB6/5557sH79emi1WmzduhV33XUX+vv78dhjj+EPf/gDsrOzk1UaEY0BYVFE\na0d/rDd+9OTp3lmeWYtrK3JRMS0HM4qyeZEbUVTSQn3evHkoKytDTU0NBEHAmjVrsHnzZhiNRixd\nuhQrVqzAqlWrIAgCvvOd78BiscSuen/wwQdj77N+/XoUFBQkq0wiSiNubwBNrdG547ZuOF2RFdrk\nMgEzJ2dHrlaflosJFp3ElRKlJ27ocgaer0kNtnNqpHs7e3xBHD7Ri6aWXnze6kBrR39sJTeDVhlb\nkrVsiiXt9/dO97bOFGxnbuhCRGnCFwih+YQTTa0OfN7iwLGT/QhHU1wuEzC9MAszi82YPTUHJRNN\nkMm4khvRhWCoE1HSBIIhNLf1oanFgaZWB46098Wmm8llAkoKjJg52YyZxWZMK8ziuupEl4ihTkQJ\nEwyFcaS9D02tDjS1ONDc1hfbb1wQgOJ8I2YWm3F5sRnTJ2VBo+KvIKJE4v8oIrpooXAYx071R3ri\nLQ4cbnPG1lMHgKI8Ay4vNmPmZDMuK8qCTqOUsFqizMdQJ6K4hcMiWjv70dTSi6ZWB7443hubLw4A\nhbn66HB6NmZMNsOgZYgTpRJDnYhGFBZFtHUNxM6JH2rthdsXjD2eb9HhqsnZmFlsxozJZmTpubsZ\nkZQY6kQUI4oiTna78fmQEHd5ArHHc7M0+NIMK2ZGh9TNRq7eRpROGOpE45goiuh0ePB59MK2Q629\ncA74Y49bTGpcUzohNqSem8WdzYjSGUOdaJyx954O8abWXjj6fbHHsvQqXDkrP3pxWzas2VoIAueK\nE40VDHWiDNfT541OMYtc3GZ3emOPGbRKVM3Mw+XR8+ITLDqGONEYxlAnykBdvR58tP8k9nzRhbau\ngdhxvUaBudNzI3PFJ5tRYNXHtiYlorGPoU6UIfyBEPZ+0YXtDSfxeYsDAKBRyVFRmoOZkyMLvhTl\nGbj0KlEGY6gTjWGiKKKlox/bG05iV2NHbLrZZUXZuLZiIv5uwVT093kkrpKIUoWhTjQGuTwB7Gw8\nhe31J3GiywUAyDKosGxeMRbOnoj86NakGrUC43s/K6LxhaFONEaEwyIOHuvB9oaT+OxwF4IhEXKZ\ngC9dZsXCiokon2qBXCaTukwikhBDnSjNdfZ68FHDSXx04CR6+iLTzwpy9bi2YiKuLpsAE1dxI6Io\nhjpRGvIHQtjzRRd2nHHR2+I5BVhYMRFTJ5o49YyIzsJQJ0oToiji2Kl+7Gg4iU8OdsBzxkVvVTPy\noFZxv3EiGhlDnUhi/W4/PmnswPaG0xe9ZRtUuP6Mi96IiM6HoU4kgXBYRGP0ord9Z1z0dm3lRJSV\n8KI3IrpwDHWiFOrs9WBHw0l8tP9kbM31wuhFb1eVT4BJx4veiOjiMdSJkswfCGHPoS5sb2hHU2sv\nAECrluO6OQVYWFGAkolGXvRGRAnBUCdKgsGL3rY3nMSuIRe9zSjKxrWVE/GlGXlQK3nRGxElFkOd\nKIH63X7sbOzAjoZ2nIhupGI2qvHlLxViweyJyDfzojciSh6GOtElCodFHDjagx0N7fjssB2hcPSi\ntxlWXFtRgPISCzdRIaKUYKgTXaROhxs79p/ER/tPnb7ozarHtRUFuKosnxe9EVHKMdSJLoAvEMLe\nES56u7ayAFMm8KI3IpIOQ50oDi2n+vFBfTt2HTwFjy8EAJg5ORvXVhRg3gwrL3ojorTAUCcaQSAY\nwu7PO7HtszbY2vsADF70NgkLZ09EHi96I6I0w1AnOkOnw41tn7Vjx/6TcHkCEABUlOZgydxCzJ6a\nw4veiChtMdSJELmCvd5mx9a9bThwtAcAYNQp8dWrinHdnALkZmslrpCI6PwY6jSuOV0+fNhwEh/s\na4vtVT59UhaWzC3El2bkQang+utENHYw1GncEUURXxzvxdbP2rDnUBdCYRFqlRxL5hZiydxCTMoz\nSF0iEdFFYajTuOHxBfHxgVPY+lkb2u2R1d4KrXosmVuIq8smQKvmfwciGtv4W4wyXmtHP7Z91oad\njR3wBUKQywRcOSsfS+YWYvqkLM4rJ6KMwVCnjBQIhvF/hzqxdW8bmtucAIAckxpfu6YYCysKkKXn\nam9ElHkY6pRRuno92LavDdvrT09Hmz01Mh2topTT0YgoszHUacwLh0U0HOnGts/asN/WDRGAQavE\njVdOxuK5hcjjdDQiGicY6jRm9Q34sb2hHds+a0d3nxcAUFpowvVzJ6FqphVKBZduJaLxhaFOY4oo\nijh8woltn7Xh06ZOhMIiVEoZFs8pwJK5hZicb5S6RCIiyTDUaUzw+IL4pDEyHe1EV2Q62sQcHZbM\nLcQ15ROh0/CfMhERfxNSWjvR6cLWz9rwceMp+PyR6WhXzMzDkrmFmDE5m9PRiIiGYKhT2gkEw9jz\nRSe27W3DFyci09HMRjW+euVkXFtZgGyDWuIKiYjSE0Od0obd6cEH+9qxvb4dfe4AAKCsxIIlcwtR\nOS0HchnXYSciGg1DnSQVFkUcONKDrXtPoCE6HU2vUeAr84tw3ZxC5Fu4ZzkRUbwY6iQJp8uHtz9p\nwdbP2mB3RqajlUw04fp5hbhiZh5USk5HIyK6UAx1SilRFPG/n7TgzY+OIRAMQ6WQ4dqKiVgyrxBT\nJpikLo+IaExjqFPKhMJhbHzvC2zb146cLA2WVhVhwewJ0GuUUpdGRJQRkhrq69atQ319PQRBQG1t\nLSoqKmKPbdmyBc888wxUKhWWLVuGW2+99byvobHL5w9hw5uN2NdsR1GeAWu/ew3C/qDUZRERZZSk\nhfru3bvR0tKCuro62Gw21NbWoq6uDgAQDoexdu1avPHGG8jOzsa9996L6upqtLa2jvgaGrv63H78\n9o8NOHqyD2VTzPj+TbORk6VFV1e/1KUREWWUpIX6zp07UV1dDQAoLS2F0+mEy+WCwWCAw+GAyWSC\nxWIBAFx11VX4+OOPcfz48RFfQ2NTp8ON37xaj06HB9eUT8CdN86EQs6paUREyZC03652ux1mszl2\n32KxoKurK3Z7YGAAx44dQyAQwK5du2C320d9DY09R9r78OhLe9Dp8GDZ1cW4e9nlDHQioiRK2YVy\noijGbguCgF/96leora2F0WjEpEmTzvuakZjNOigSvBuX1cpNQS7V7oOn8OtXPkMgEML3b6nAjdeU\nnPUctnNqsJ1Th22dGmznkSUt1PPy8mC322P3Ozs7YbVaY/fnz5+PTZs2AQAef/xxFBYWwufzjfqa\nc3E43Amt22o18lzvJfpgXxv+691DUMpluP/m2Zg7PfesNmU7pwbbOXXY1qnBdh79Q03SxkIXLFiA\nd999FwDQ2NiIvLy8YefG77nnHnR3d8PtdmPr1q24+uqrz/saSm+iKOKND4/gxXcOQa9R4l++NRdz\np4/+oYyIiBInaT31efPmoaysDDU1NRAEAWvWrMHmzZthNBqxdOlSrFixAqtWrYIgCPjOd74Di8UC\ni8Vy1mtobAiGwnjxnSZ8tP8UrNka/POKOVzilYgoxQQxnhPXaSzRwzAc2rlwHl8Qz/zpAA4c7cGU\nCUb8YHklsvSqUV/Ddk4NtnPqsK1Tg+08+vA7V5SjS+J0+fDkHxvQ0tGPitIcfO8b5VCruG47EZEU\nGOp00U52D+CJV+thd3qxqHIibvvKDG6PSkQkIYY6XZTDJ3rxH681YMAbxD8sLMHXF0yBIAhSl0VE\nNK4x1OmC7TnUhWf/0ohQSMRdX52JaysKpC6JiIjAUKcL9Nc9J7Dp/S+gUsrxwPLZmD01R+qSiIgo\niqFOcQmLIl7fZsPbu1ph0qvw4PIK7n9ORJRmGOp0XoFgGP/vfz/HJwc7kG/R4Z9XVMKarZW6LCIi\nOgNDnUbl9gbx9OYGNLX2orTQhP/vlgoYdaPPQSciImkw1GlEPX1ePPnHepzoGsDc6bm47+/LoFJy\nDjoRUbpiqNM5nehy4YlX6+Ho9+H6eYX4dvVlkMk4ZY2IKJ0x1OksTS0OPLV5Pzy+IL55XSluvHIy\n56ATEY0BDHUaZtfBDjz/1kGIInDv12fh6rIJUpdERERxYqgTgMi2qe/uPo5XtzZDq5bj/ptmY9YU\ni9RlERHRBWCoE8JhEa/87TC2/N8JZBtUeGjFHBTlcR97IqKxhqE+zgWCITz7l4PYc6gLBbl6PLS8\nEjlZGqnLIiKii8BQH8dcngCeer0Bh084MaMoGw/cMht6jVLqsoiI6CIx1Mcpu9ODJ16tx8luN+Zf\nnoe7l82CUsFtU4mIxjKG+jjU2tGPJ16th3PAj6/ML8LyJdMg45Q1IqIxj6E+zjQe7cHTb+yH3x/C\nt748HUuvKJK6JCIiShCG+jjy0f6T+MPbTRAEAd/7h3JUzcyTuiQiIkoghvo4IIoi3trZgs0fHoFe\no8A/3VKBy4qypS6LiIgSjKGe4ULhMDa+fxjbPmtDjkmNh1bMQUGuXuqyiIgoCRjqGcwXCGHDnxux\nr9mOojwDHlxeCbNRLXVZRESUJAz1DNXn9uM/XmvAkfY+lE0x4/s3zYZWzb9uIqJMxt/yGajT4cZv\nXq1Hp8ODa8on4M4bZ0Ih5xx0IqJMF1eoi6LIrTfHiKMn+/DkH+vR7w5g2dXFuHnRVP7dERGNE3F1\n35YsWYInnngCx48fT3Y9dAnqm+1Yv2kvXJ4AbvvKDNyyuJSBTkQ0jsQV6n/84x9htVpRW1uLu+66\nC3/5y1/g9/uTXRtdgA/r2/HU6/sBEXjg5tlYMrdQ6pKIiCjFBFEUxQt5QUtLC1avXg2bzYaamhp8\n//vfh1ot3RXVXV39CX0/q9WY8PdMJlEU8ecdR/HmR8dg0Crxg29WoLQwS+qyzmustfNYxXZOHbZ1\narCdI20wkrivnvr000+xevVq3HvvvZg3bx42bdoEk8mEH/zgBwkpki7O/3x8DG9+dAzWbA0eue1L\nYyLQiYgoOeK6UG7p0qUoLCzEihUr8POf/xxKZWR7ztLSUmzZsiWpBdLIRFHEh/Xt0KkVqL2tCll6\nldQlERGRhOIK9eeeew6iKGLKlCkAgIMHD2LWrFkAgE2bNiWtOBrdia4BdPf5cOWsfAY6ERHFN/y+\nefNmbNiwIXb/2Wefxb//+78DAK+ullB9sx0AUFmaI3ElRESUDuIK9V27duGXv/xl7P6TTz6JPXv2\nJK0oik+9zQ6ZIKB8KkOdiIjiDPVAIDBsCtvAwACCwWDSiqLz63P7caStD9MmZcGgVUpdDhERpYG4\nzqnX1NTgq1/9KsrLyxEOh7F//3488MADya6NRrHf1g0RQOU09tKJiCgirlBfvnw5FixYgP3790MQ\nBKxevRoGgyHZtdEoTp9Pz5W4EiIiShdxz1N3u92wWCwwm804cuQIVqxYkcy6aBTBUBgHjvYgL1uL\niTk6qcshIqI0EVdP/Re/+AU++ugj2O12TJ48GcePH8eqVauSXRuN4NDxXnj9ISysyOHsAyIiiomr\np75//368/fbbmDlzJl5//XW88MIL8Hg8ya6NRhAbep/GoXciIjotrlBXqSILmwQCAYiiiPLycuzd\nuzephdG5iaKI+mY7NCo5ZhRlS10OERGlkbiG30tKSrBx40ZUVVXhrrvuQklJCfr7x/eC+lI52e1G\nV68XVTOsUMjjviSCiIjGgbhC/Wc/+xmcTidMJhPeeustdHd347777kt2bXQO9TYOvRMR0bnFFerr\n1q3DI488AgD4+te/ntSCaHT1zd0QAMzm0rBERHSGuMZv5XI5du7cCZ/Ph3A4HPtDqeXyBNB8womp\nhSaYdNzAhYiIhourp/7HP/4RL774IkRRjB0TBAGff/550gqjsx040o2wKHLBGSIiOqe4Qp2bt6SH\nels3AJ5Pl4IoigiEgxAAyAQZBEGAAIHrBBBRWokr1H/729+e8/gPfvCDhBZDIwuFw9hv60aOSY1J\nVr3U5YxZwXAQ7qAH7oAH7qAbAwF39LYH7oAbA9Gv7iFfB6Jfw+LZp5wGg10myCBAgEwQIEAG2ZnH\nYrdl0eeP8LzoMeGM18qGfJ9hzxvynqdfK0AWva1rVUP0y6BRaKCRq6FRqIfflmuiX9VQyVWQCZxR\nES9RFBEMB+EL+REIB2AKqqUuiSi+UJfL5bHbgUAAn376KWbNmnXe161btw719fUQBAG1tbWoqKiI\nPbZx40a8+eabkMlkKC8vxyOPPIKOjg7U1tbC7/cjHA5j9erVKC8vv4gfK/M0n3DC7QviyrL8cd87\nFEUR3pAXA9FgHgzlgYAbnoAHA0H3WYHsjh73h/zn/wZRMkEGvUIHvVIPqzYXGoU69v3DYhgiIl/D\nohi7HfsqighDhDh4WwwjPPj88LmfF/s65P1SSYAAdSzszw5/tUINbfT44PO00eeoo8/RRh9TyhRp\n8e80MsISgC/khz/kj3wND7kd8sMXCsAfu+2Db9jjgWGvG3zN4DER4rDvp1Noka3OQrYmC2Z1Nszq\nrCH3I7c1Co1ErUHjQVyhfuaObKFQCP/0T/806mt2796NlpYW1NXVwWazoba2FnV1dQAAl8uF559/\nHu+99x4UCgVWrVqFffv24d1338XSpUtRU1ODvXv34oknnsDzzz9/kT9aZqlvjg69Z9D59EA4OCx8\nPYMhHDt2OrQHgqcD2xP0XlDgaeQa6JVa5GtzoVPqoFNooVPqoI/d1kKn0EEf/Tp4Xy1XSR5MQz8Q\nDP0wcWb4D/uAIYoII/KBIStbg5N2B7xBL7whH7xBH7whL3xBHzwhb+R+0AtfyAfPkMdcgQHYPd0I\niqGLqlsmyKCNfhA41weEYR8chj1HA5VchWA4GAnPaJD6Qj74Q4GzQvVcQesP+eGLHg+EAmcF78X+\nPGq5CiqZChq5GiaVESq5KnJWNlKCAAAd3ElEQVRMroJSpkBA8KGjvwc93l60D5wa8b00cs2wkM9W\nR29rTt/WKrSS/9ujsSmuUD9TMBhEa2vrqM/ZuXMnqqurAQClpaVwOp1wuVwwGAxQKpVQKpVwu93Q\n6XTweDzIysqC2WxGb28vAKCvrw9ms/liystI9TY7VEoZLi9Ov1XkwmI4FsyRPwNDbrsxEHSf9Zgn\n6IHvAnrNCkEOnVIHo9KAfF0edAptJJSV2lhIx45Fg1mv0EGr0EAuk5//G6QpQRAgF+S42J/Amm2E\nNmC66O8fCAfhi4Z95AOBb8gHhDO/+s768OAN+uDwOeEd6EhIuI5GLshjQatVaJCtMkXvq4cFsEqu\nhFqmOuOYKhbaaoXqrMcVsvP/qrRajejqiizK5Ql64fQ54fA50et1onfw9uAfrxOnBjpGfC+VTBkN\n+ezh4a85fdug1DP46SxxhfrixYuH/eNxOp246aabRn2N3W5HWVlZ7L7FYkFXVxcMBgPUajXuv/9+\nVFdXQ61WY9myZSgpKcGdd96Jb37zm/jTn/4El8uFl19++SJ/rMzS4XDjZLcbc6fnQqlIXkCJogh/\nOHB2KMfC+czjA7Gh73h/YWvkauiVOhSY8qGGJhLKwwI52oseEtQ6pQ4qmZK/wCSglCmgVClgwKVd\nxzE4DO6Jhf2Q0YHobW909MAf8kMhUwwP2jPCV31mEMtVafXhTauInIqYoM8f8Tn+kD8W8g7v6cB3\nDAn+TnfziK9XyBTIVpmG9PCzzxgByIZRped1EuNMXKG+adOm2G1BEGAwGGAyXdin/6HT4VwuFzZs\n2IB33nkHBoMBd9xxB5qamvC3v/0NN954I773ve9h69atWL9+PZ5++ulR39ds1kGR4KCzWo0Jfb9L\n9fHnnQCABXMmxV1bKBzCgN+Nfv8A+n0DcPld6PcNoN8/AJd/AK4htyPHXXD5BhAIB+N6f7kgg0Ft\ngFmXhcnqAhhUehhVehjUBhhVehjVehhUkT9GdfQxlR4K+UUNDtElSLd/z5nsQtu6EKMvIhUIBeDw\nONHtcaDb3YsejwN2twM97l50eyJfbb3HRvxQLRdksGizYdGZkTPka47ODIs2G2ZtFrI1Jijlyguq\nW2r8Nz2yuH7Dejwe/PnPf8bDDz8MAFi9ejVWrVqF6dOnj/iavLw82O322P3Ozk5YrVYAgM1mQ1FR\nESwWCwCgqqoKBw4cwN69e/Hggw8CABYsWICf/exn563N4XDH8yPEbegQWrr4aF8bAGBqvn5YbcFw\nEJ927MOhnubTvevoULcnGP8uelqFBnqFDhP1E6CPnmvWK/Wx2wbF8Ps6pQ4auTr+nrMfCPgBB07X\nlI7tnInYzqmTrLYWoEYuJiBXNwHQAWd+DgiFQ3D6+87d44/eP9x9FIdGuQ5Fr9DBpDbCpDLCpDIh\nS21Elip6X22K3FabLuz/fZLw3/ToH2riXvt96PS1W265BT//+c/x0ksvjfiaBQsW4KmnnkJNTQ0a\nGxuRl5cHg8EAACgsLITNZoPX64VGo8GBAwewePFiFBcXo76+HuXl5WhoaEBxcXG8P2PG8viC+OJ4\nL6ZMMCLbELn62hv04eP2Xfjr8e3o9Tljz1UIcuiVOpjVWZhkmDgsoCPD20OCesjxdBq2JKILI5fJ\nYdGYYdGYgaxzPycshtHn748N6zt8Tjh8vejz9aPP3w+nvx9OXx9OjnKeHwCUMmUs4CNfox8CosdM\nKiOy1EYYlBz2l0pcoR4KhVBVVRW7X1VVNWw4/VzmzZuHsrIy1NTUQBAErFmzBps3b4bRaMTSpUtx\n99134/bbb4dcLsfcuXNRVVWFyZMn45FHHsE777wDALH15sezA0d7EAqLqJyWC5d/ANtOfIQPTnwE\nd9ADlUyJJUULsbDgKmSrs9Liam0iSj8yQRa7wA6jnDkNhALo87vQ5++D09+PPl9fJPR9/UOO9eNY\nX+uoM1BkggxGpf50+KtMMA32/mPHIn/G2tB/uhPE86UzgPvuuw+LFy/GlVdeiXA4jO3bt2PXrl3Y\nsGFDKmocVaKHYdJtaOe5/zmInV8cxTXXu7G/dx8C4QD0Ch0WFy3A4knXwKAcmwvRpFs7Zyq2c+qM\np7YOi2G4AgOxsO/zRXr7ff6+6LHIBwKnvx+BcGDU99IptKd7+dHef5bqdK9/8HSARq6BIAjjqp1H\ncsnD77/85S/x+OOPx65GnzdvHn75y18mpjoaUVv/SezzbYGmsg17e0SY1dn48uRFuKZgPtRybuhC\nRNKQCbJYTxsoGPF5gwtFxUI/GvRn9v77ff2jTvEDIkP/JpURJq0eclEBtVw9ZBaEOjorQh2bHTH6\nY+qMPe0YV6hbLBbce++9mDJlCgDg4MGDsYvcKPGOOFvwXstW7LcfBMyAJpyN5bNuQFX+nLjmyxIR\npQNBEKBVaKFVaJGvzxv1uYFwMHqO/4wh/8Gef/R2e38nfEHfJdemiK1rcDr0VXLliB8CVNH1C9QK\n9fA1DYY8TyWTfmplXAnxxBNPoLOzM9Y7f/bZZzFp0iT88Ic/TGpx44koimjsbsL7rdvQ3HsUAGBC\nHrq+mIS7r78e8yaO/h+CiGgsU8oUyNGakaMdfdExq9WIjk5nZAnfsB++YHR536FL/Q77eubjZz/f\nFRhAt7cn7im95/s5hi54pJarcc3EK7Cg8MpLfu94xBXqu3btwiuvvBK7/+STT+Jb3/pW0ooaT0Lh\nEPZ2NuD91m1oc50EAMzKmYEbJi/Bf23uhNzlRVnJ6HNZiYjGE5kgiywvDDWQwDORYTE86oeA2JLF\nwdP7BJz9oeL0/T5fP3whO04YJyWuyPOIK9QDgQD8fj9UqkjrDQwMIBi89E8045k/FMAnJz/FltYP\n0e3tgQABVflzsHTydZhkLIDd6UFb1zFUlOZArczMcz9EROlEJshiqwGOVXGFek1NDb761a+ivLwc\n4XAY+/fvxx133JHs2jKSO+DBh207sfX4drgCA1DIFLi28GpUT16EXO3pHvnpDVzYSyciovjEFerL\nly/HlClT4HA4IAgCrr/+emzYsAF33nlnksvLHL0+J7Ye34EdbZ/AG/JBI9fghuIlWFK0MHoF6XD1\ntshqfJXTMmdXNiIiSq64Qv3RRx/Fjh07YLfbMXnyZBw/fhyrVq1Kdm0ZocPdhS0tH2D3qT0IiiGY\nVEb83ZQvY2HhldAqtOd8jdcfRFOLA0V5BlhMY3cYiIiIUiuuUG9oaMDbb7+N2267DS+99BIOHDiA\n999/P9m1jWmtfSfwXstW7Os6ABEirNocLJ18HeZPmHfeFZQOHnMgGBJROY1D70REFL+4Qn3wArlA\nIABRFFFeXo7169cntbCxSBRFHHI04/2WbWhyHAYAFBkLcUPxEsyxlse9FnJ9M4feiYjowsUV6iUl\nJdi4cSOqqqpw1113oaSkBP3943uZvqHCYhj1XY14r2UrWvtPAAAuM0/DDcXXYaZ5+gWtxx4WRTTY\numHUKVEy8cK2tyUiovEt7l3anE4nTCYT3nrrLXR3d+O+++5Ldm1pLxAO4tNTe/F+6zZ0uu0QIGCO\ntRw3FC9Bsanoot6z5VQ/nAN+LJg9ATJuzkJERBcgrlAXBAHZ2dkAgK9//etJLWgs8Aa92NG+C39r\n3Q6nvw9yQY5rJl6B6smLz7sU4vnEht5LOfROREQXhguJX4B+vwvbju/AB2074Ql6oJar8OWiRbh+\n8rWRLQ0ToL65G3KZgLISrq1PREQXhqEeh25PD7a0foidJ3cjEA7CoNTjayVfwaJJV0Ov1CXs+zj6\nfWjp6EfZFDO0av7VEBHRhWFyjKLNdRLvt2zDns56hMUwLBpzZOvTiVdAlYStTwcXnKngVe9ERHQR\nGOrn0Nx7FO+3bMWB7iYAQIF+ApYWX4cv5VUmdVu9hsGlYRnqRER0ERjqQ7T2ncB/NLyFQ3YbAKA0\nawpuKF6CspyZFzQt7WL4AyEcPNaDglw98rLPvdIcERHRaBjqQ2xp/QCH7DaU51yOpcXXYVp2Scq+\n9+ctDviDYW7gQkREF42hPkTNjJtx1/zlENyJP19+PvU2Dr0TEdGliW/d0nFCp9QiT5/6nrIoiqhv\ntkOvUaC0kKvIERHRxWGop4HjnS44+n2YXZoDuYx/JUREdHGYIGmAq8gREVEiMNTTQL2tGzJBwOyp\nXEWOiIguHkNdYs4BP4629+GyoizoNKPvs05ERDQahrrEGmx2iAAqOPRORESXiKEuscFV5OZMZ6gT\nEdGlYahLKBAM48CxHuSbtZhgSdzGMEREND4x1CV06LgDPn+IC84QEVFCMNQlVM8NXIiIKIEY6hIZ\nXEVOq5Zj+qQsqcshIqIMwFCXSLt9AHanF+UlOVDI+ddARESXjmkikdMbuHBXNiIiSgyGukTqm+0Q\nBGD2VIY6ERElBkNdAi5PAM1tTpQWZsGoS/02r0RElJkY6hLYb+uGKAKVpeylExFR4jDUJVBvi+zK\nNodT2YiIKIEY6ikWDIWx/0gPcrM0KMjVS10OERFlEIZ6ih0+4YTHF0RlaS4EQZC6HCIiyiAM9RSr\nb44MvVdO5/l0IiJKLIZ6itXbuqFWyjGjyCx1KURElGEY6il0qseNjh43ykosUCrY9ERElFhMlhSK\nDb1zFTkiIkoChnoKDYZ6RSmnshERUeIx1FPE7Q3g8AknSiaakKXnKnJERJR4DPUUOXC0B6GwyKF3\nIiJKGoZ6igwOvXMVOSIiShaGegqEwmE02LphNqpRlGeQuhwiIspQDPUUsLX1YcAbRGVpDleRIyKi\npFEk883XrVuH+vp6CIKA2tpaVFRUxB7buHEj3nzzTchkMpSXl+ORRx4BADz//PN48803oVAosGbN\nmmGvGasGN3Cp5NA7ERElUdJCfffu3WhpaUFdXR1sNhtqa2tRV1cHAHC5XHj++efx3nvvQaFQYNWq\nVdi3bx/0ej3eeustvP766zh06BD++te/ZkSoNzR3Q6WQ4fJiriJHRETJk7RQ37lzJ6qrqwEApaWl\ncDqdcLlcMBgMUCqVUCqVcLvd0Ol08Hg8yMrKwvvvv48bb7wRCoUCZWVlKCsrS1Z5KdPV60GbfQCV\npTlQKeVSl0NERBksaaFut9uHhbLFYkFXVxcMBgPUajXuv/9+VFdXQ61WY9myZSgpKUFbWxvkcjnu\nvvtuBINBrF69GjNnzhz1+5jNOigUiQ1Lq9WYsPf6pKkLALBw7qSEvm8mYHukBts5ddjWqcF2HllS\nz6kPJYpi7LbL5cKGDRvwzjvvwGAw4I477kBTUxNEUUQoFMJzzz2HPXv24JFHHsHrr78+6vs6HO6E\n1mm1GtHV1Z+w9/uovg0AMDXfkND3HesS3c50bmzn1GFbpwbbefQPNUkL9by8PNjt9tj9zs5OWK1W\nAIDNZkNRUREsFgsAoKqqCgcOHEBubi6mTp0KQRBQVVWFtra2ZJWXEh5fEIdaHZicb4DZqJa6HCIi\nynBJm9K2YMECvPvuuwCAxsZG5OXlwWCIzNEuLCyEzWaD1+sFABw4cABTpkzBokWLsGPHDgCR4J84\ncWKyykuJg8d6EAyJqORa70RElAJJ66nPmzcPZWVlqKmpgSAIWLNmDTZv3gyj0YilS5fi7rvvxu23\n3w65XI65c+eiqqoKAPDhhx9i5cqVAICf/OQnySovJeqbuwEAc6Yz1ImIKPkEcejJ7jEo0edWEnW+\nJiyK+OendgCCgN88sAAyLjozDM+LpQbbOXXY1qnBdh79nDpXlEuSoyf70OcOoKI0h4FOREQpwVBP\nktjQO1eRIyKiFGGoJ0lDsx0KuYBZU7iKHBERpQZDPQl6+rxo7XRh5mQzNKqULQVARETjHEM9Cept\nkaF3buBCRESpxFBPgvrm6K5spTkSV0JEROMJQz3BfIEQPm9xoNCqR262VupyiIhoHGGoJ9jnxxwI\nBMNcRY6IiFKOoZ5g9bbI0DunshERUaox1BNIFEXUN9th0CoxtcAkdTlERDTOMNQTqLXDhV6XH7On\n5kAm4ypyRESUWgz1BBq86p0buBARkRQY6glUb7NDLhNQNsUidSlERDQOMdQTpNflw9GT/bisKBs6\nDVeRIyKi1GOoJ0gDV5EjIiKJMdQTJLaK3DSuIkdERNJgqCdAIBhC47EeTLDokG/WSV0OERGNUwz1\nBGhq7YU/EGYvnYiIJMVQT4DYVDaeTyciIgkx1C/R4CpyOrUCpYVZUpdDRETjGEP9ErV1DaC7z4fy\nqRYo5GxOIiKSDlPoEnEDFyIiShcM9Uu0r9kOQQDKp/IiOSIikhZD/RL0uf040taH6YVZMGiVUpdD\nRETjHEP9Euy3dUMEUMkNXIiIKA0w1C9B/eDSsKUMdSIikh5D/SIFQ2EcONINa7YGE3O4ihwREUmP\noX6RvjjeC68/hMppuRAEQepyiIiIGOoXq76Zu7IREVF6YahfhMFV5NQqOWYUZUtdDhEREQCG+kU5\n1eNGZ68H5SVcRY6IiNIHE+kiDA69cxU5IiJKJwz1i7Cv2Q4BwGyuIkdERGmEoX6BXJ4Amk84MbXA\nBJNeJXU5REREMQz1C3TgaDfCosir3omIKO0w1C9QA6eyERFRmmKoX4BQOIz9R7phMakxyaqXuhwi\nIqJhGOoXoPmEEwPeIFeRIyKitMRQvwDcwIWIiNIZQ/0C1DfboVLKcHkxV5EjIqL0w1CPU4fDjZPd\nbswqtkCpkEtdDhER0VkY6nEavOp9znQOvRMRUXpiqMdpX7MdAFeRIyKi9MVQj4PHF8QXx3tRPMEI\ns1EtdTlERETnxFCPQ+PRHoTCIjdwISKitMZQj0N9dOi9chqH3omIKH0x1M8jHBZRb+tGlkGFyflG\nqcshIiIaEUP9PI6c7IPLE0BlaS5kXEWOiIjSWFJDfd26dVi5ciVqamrQ0NAw7LGNGzdi5cqV+Na3\nvoVHH3102GN2ux1XXHEFdu3alczy4sKhdyIiGiuSFuq7d+9GS0sL6urq8Oijjw4LbpfLheeffx4b\nN27Eyy+/DJvNhn379sUef+yxx1BUVJSs0i5IfbMdCrkMs4otUpdCREQ0qqSF+s6dO1FdXQ0AKC0t\nhdPphMvlAgAolUoolUq43W4Eg0F4PB5kZWXFXqfX63HZZZclq7S42Z0enOgawOXFZqhVXEWOiIjS\nW9JC3W63w2w2x+5bLBZ0dXUBANRqNe6//35UV1djyZIlqKysRElJCfx+P373u9/hoYceSlZZF6Qh\nuoHLHA69ExHRGKBI1TcSRTF22+VyYcOGDXjnnXdgMBhwxx13oKmpCVu2bMHy5cthMpnifl+zWQdF\ngtdit1ojV7kfbO0FAFw3vxhWsy6h34NOtzMlF9s5ddjWqcF2HlnSQj0vLw92uz12v7OzE1arFQBg\ns9lQVFQEiyVynrqqqgoHDhzAjh07EA6HsXHjRrS2tqKhoQG//e1vMX369BG/j8PhTmjdVqsRXV39\n8PqDaDjchUlWA4RgCF1d/Qn9PuPdYDtTcrGdU4dtnRps59E/1CRt+H3BggV49913AQCNjY3Iy8uD\nwWAAABQWFsJms8Hr9QIADhw4gClTpuCVV17Bq6++ildffRXXXXcd1qxZM2qgJ9PnxxwIhkTMmc6h\ndyIiGhuS1lOfN28eysrKUFNTA0EQsGbNGmzevBlGoxFLly7F3Xffjdtvvx1yuRxz585FVVVVskq5\nKPW26FS2Ui4NS0REY4MgDj3ZPQYlehjGajWio7MPDz/9EcKiiCceWAiZjIvOJBqH0FKD7Zw6bOvU\nYDtLNPw+lrWc6odzwI+K0hwGOhERjRkM9XOIrSLHoXciIhpDGOrnUN/cDblMQFkJV5EjIqKxg6F+\nhm6nBy0d/Zg5ORtadcqm8RMREV0yhvoZ/u/zDgBAxTQOvRMR0djCUD/D7sZIqFeWcn46ERGNLQz1\nIfyBEPYd7sLEHB3yuCwsERGNMQz1IZpaHfAHQpjDoXciIhqDGOpD1DdHdmWrZKgTEdEYxFAfQgRQ\nkKtHaWH8u8QRERGlC87ZGuLWGy6DNdeI7m6X1KUQERFdMPbUh5AJApeFJSKiMYuhTkRElCEY6kRE\nRBmCoU5ERJQhGOpEREQZgqFORESUIRjqREREGYKhTkRElCEY6kRERBmCoU5ERJQhGOpEREQZgqFO\nRESUIQRRFEWpiyAiIqJLx546ERFRhmCoExERZQiGOhERUYZgqBMREWUIhjoREVGGYKgTERFlCIb6\nEOvWrcPKlStRU1ODhoYGqcvJWI899hhWrlyJW265Be+9957U5WQ0r9eL6upqbN68WepSMtabb76J\nv//7v8fNN9+Mbdu2SV1ORhoYGMADDzyA2267DTU1Ndi+fbvUJaUthdQFpIvdu3ejpaUFdXV1sNls\nqK2tRV1dndRlZZxPPvkEhw8fRl1dHRwOB2666SbccMMNUpeVsZ555hlkZWVJXUbGcjgc+N3vfofX\nX38dbrcbTz31FK677jqpy8o4b7zxBkpKSvDwww+jo6MDd9xxB9555x2py0pLDPWonTt3orq6GgBQ\nWloKp9MJl8sFg8EgcWWZ5YorrkBFRQUAwGQywePxIBQKQS6XS1xZ5rHZbGhubmbIJNHOnTtx9dVX\nw2AwwGAwYO3atVKXlJHMZjMOHToEAOjr64PZbJa4ovTF4fcou90+7B+KxWJBV1eXhBVlJrlcDp1O\nBwB47bXXsGjRIgZ6kqxfvx7/+q//KnUZGe3EiRPwer347ne/i29/+9vYuXOn1CVlpGXLlqG9vR1L\nly7Frbfeih/96EdSl5S22FMfAVfPTa4tW7bgtddewwsvvCB1KRnpT3/6E+bMmYOioiKpS8l4vb29\nePrpp9He3o7bb78dW7duhSAIUpeVUf785z+joKAAzz//PJqamlBbW8vrREbAUI/Ky8uD3W6P3e/s\n7ITVapWwosy1fft2/P73v8dzzz0Ho9EodTkZadu2bTh+/Di2bduGU6dOQaVSYcKECbjmmmukLi2j\n5OTkYO7cuVAoFJg8eTL0ej16enqQk5MjdWkZZe/evVi4cCEAYObMmejs7ORpuxFw+D1qwYIFePfd\ndwEAjY2NyMvL4/n0JOjv78djjz2GDRs2IDs7W+pyMtaTTz6J119/Ha+++iqWL1+O73//+wz0JFi4\ncCE++eQThMNhOBwOuN1unu9NguLiYtTX1wMA2traoNfrGegjYE89at68eSgrK0NNTQ0EQcCaNWuk\nLikj/e///i8cDgcefPDB2LH169ejoKBAwqqILk5+fj6+8pWvYMWKFQCAH//4x5DJ2FdKtJUrV6K2\ntha33norgsEgfvrTn0pdUtri1qtEREQZgh8piYiIMgRDnYiIKEMw1ImIiDIEQ52IiChDMNSJiIgy\nBEOdiJJm8+bN+OEPfyh1GUTjBkOdiIgoQ3DxGSLCSy+9hLfffhuhUAhTp07FPffcg/vuuw+LFi1C\nU1MTAOCJJ55Afn4+tm3bht/97nfQaDTQarVYu3Yt8vPzUV9fj3Xr1kGpVCIrKwvr168HALhcLvzw\nhz+EzWZDQUEBnn76aa6NTpQk7KkTjXMNDQ14//33sXHjRtTV1cFoNOLjjz/G8ePHcfPNN2PTpk2Y\nP38+XnjhBXg8Hvz4xz/GU089hZdeegmLFi3Ck08+CQD4l3/5F6xduxb//d//jSuuuAIffPABAKC5\nuRlr167F5s2bcfjwYTQ2Nkr54xJlNPbUica5Xbt2obW1FbfffjsAwO12o6OjA9nZ2SgvLwcQWUb5\nxRdfxLFjx5CTk4MJEyYAAObPn49XXnkFPT096Ovrw2WXXQYAuPPOOwFEzqnPnj0bWq0WQGRZ1f7+\n/hT/hETjB0OdaJxTqVS4/vrr8ZOf/CR27MSJE7j55ptj90VRhCAIZw2bDz0+0orTZ268wZWpiZKH\nw+9E49y8efPw4YcfYmBgAACwceNGdHV1wel04uDBgwAiW1/OmDEDU6ZMQXd3N9rb2wEAO3fuRGVl\nJcxmM7Kzs9HQ0AAAeOGFF7Bx40ZpfiCicYw9daJxbvbs2fjHf/xH3HbbbVCr1cjLy8OVV16J/Px8\nbN68Gb/61a8giiJ+85vfQKPR4NFHH8VDDz0ElUoFnU6HRx99FADw61//GuvWrYNCoYDRaMSvf/1r\nvPfeexL/dETjC3dpI6KznDhxAt/+9rfx4YcfSl0KEV0ADr8TERFlCPbUiYiIMgR76kRERBmCoU5E\nRJQhGOpEREQZgqFORESUIRjqREREGYKhTkRElCH+f/t+MzZNQPk3AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "DtUaOoZf-9RC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Bag-of-Words with Hidden Layer"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "pum8HYkz-9RD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "outputId": "499aab22-0b5e-465d-ce61-7a56a43aff14"
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Activation, Dense, Input\n",
        "from keras.models import Model\n",
        "import numpy as np\n",
        "\n",
        "class BOWHiddenSentimentModel(object):\n",
        "    def __init__(self, N=64):\n",
        "        bow = Input(shape=(len(vocab),), name='bow_input')\n",
        "        hidden = Dense(N, activation='tanh')(bow)\n",
        "        sentiment = Dense(1, activation='sigmoid')(hidden)\n",
        "\n",
        "        self.model = Model(inputs=[bow], outputs=[sentiment])\n",
        "        self.model.summary()\n",
        "        self.model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    def train(self, X, y, X_val, y_val):\n",
        "        print('Fitting...')\n",
        "        return self.model.fit(np.array(X), y, validation_data=(np.array(X_val), y_val), epochs=10, verbose=1)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.model.predict(np.array(X))\n",
        "    \n",
        "sentiment = BOWHiddenSentimentModel()\n",
        "history = sentiment.train(X_bow_train, y_train, X_bow_val, y_val)\n",
        "best_train_history(history)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bow_input (InputLayer)       (None, 5000)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 64)                320064    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 320,129\n",
            "Trainable params: 320,129\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Fitting...\n",
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/10\n",
            "25000/25000 [==============================] - 10s 385us/step - loss: 0.3299 - acc: 0.8580 - val_loss: 0.2998 - val_acc: 0.8702\n",
            "Epoch 2/10\n",
            "25000/25000 [==============================] - 8s 307us/step - loss: 0.2349 - acc: 0.9030 - val_loss: 0.3294 - val_acc: 0.8620\n",
            "Epoch 3/10\n",
            "25000/25000 [==============================] - 8s 306us/step - loss: 0.2026 - acc: 0.9196 - val_loss: 0.3192 - val_acc: 0.8659\n",
            "Epoch 4/10\n",
            "25000/25000 [==============================] - 8s 304us/step - loss: 0.1775 - acc: 0.9283 - val_loss: 0.3525 - val_acc: 0.8586\n",
            "Epoch 5/10\n",
            "25000/25000 [==============================] - 8s 308us/step - loss: 0.1511 - acc: 0.9424 - val_loss: 0.3766 - val_acc: 0.8606\n",
            "Epoch 6/10\n",
            "25000/25000 [==============================] - 8s 309us/step - loss: 0.1225 - acc: 0.9540 - val_loss: 0.4036 - val_acc: 0.8565\n",
            "Epoch 7/10\n",
            "25000/25000 [==============================] - 8s 311us/step - loss: 0.0913 - acc: 0.9679 - val_loss: 0.4564 - val_acc: 0.8547\n",
            "Epoch 8/10\n",
            "25000/25000 [==============================] - 10s 382us/step - loss: 0.0629 - acc: 0.9802 - val_loss: 0.5052 - val_acc: 0.8536\n",
            "Epoch 9/10\n",
            "25000/25000 [==============================] - 9s 341us/step - loss: 0.0400 - acc: 0.9908 - val_loss: 0.5871 - val_acc: 0.8508\n",
            "Epoch 10/10\n",
            "25000/25000 [==============================] - 8s 304us/step - loss: 0.0240 - acc: 0.9956 - val_loss: 0.6373 - val_acc: 0.8514\n",
            "Accuracy (epoch 1): 0.8580 train, 0.8702 val\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1UNgXwqO-9RQ",
        "colab_type": "code",
        "colab": {},
        "outputId": "4efd1a02-f8f8-464a-98bd-b0ff3127783b"
      },
      "cell_type": "code",
      "source": [
        "plot_train_history(history)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nY0yjLCyMMcY06v8D4wNHJaDdIHkAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fbd1746bcd0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "K834B3Rx-9Ra",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Overfitting!  Ideas:\n",
        "  * Apply dropout\n",
        "  * L2-regularize the weights of the hidden layer\n",
        "  * Different N values?\n",
        "  * Multiple hidden layers, with different activation?\n",
        "  * tf-idf !\n",
        "  \n",
        "Can anyone fix the bag-of-words model?"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "BvC5o3eH-9Rc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        },
        "outputId": "9b10f7f0-9118-4735-87ad-901117a72b3d"
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Activation, Dense, Dropout, Input\n",
        "from keras.models import Model\n",
        "from keras import regularizers\n",
        "import numpy as np\n",
        "\n",
        "class BOWHiddenRegularizedSentimentModel(object):\n",
        "    def __init__(self, N=128):\n",
        "        bow = Input(shape=(len(vocab),), name='bow_input')\n",
        "        hidden = Dropout(0.5)(Dense(N, kernel_regularizer=regularizers.l2(1e-3))(bow))\n",
        "        sentiment = Dense(1, activation='sigmoid')(hidden)\n",
        "\n",
        "        self.model = Model(inputs=[bow], outputs=[sentiment])\n",
        "        self.model.summary()\n",
        "        self.model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    def train(self, X, y, X_val, y_val):\n",
        "        print('Fitting...')\n",
        "        return self.model.fit(np.array(X), y, validation_data=(np.array(X_val), y_val), epochs=10, verbose=1)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.model.predict(np.array(X))\n",
        "    \n",
        "sentiment = BOWHiddenRegularizedSentimentModel()\n",
        "history = sentiment.train(X_bow_train, y_train, X_bow_val, y_val)\n",
        "best_train_history(history)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bow_input (InputLayer)       (None, 5000)              0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 128)               640128    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 640,257\n",
            "Trainable params: 640,257\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Fitting...\n",
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/10\n",
            "25000/25000 [==============================] - 17s 680us/step - loss: 0.4987 - acc: 0.8487 - val_loss: 0.4607 - val_acc: 0.8620\n",
            "Epoch 2/10\n",
            "25000/25000 [==============================] - 16s 657us/step - loss: 0.4366 - acc: 0.8737 - val_loss: 0.4451 - val_acc: 0.8679\n",
            "Epoch 3/10\n",
            "25000/25000 [==============================] - 16s 658us/step - loss: 0.4208 - acc: 0.8817 - val_loss: 0.4482 - val_acc: 0.8606\n",
            "Epoch 4/10\n",
            "25000/25000 [==============================] - 17s 663us/step - loss: 0.4154 - acc: 0.8789 - val_loss: 0.4423 - val_acc: 0.8627\n",
            "Epoch 5/10\n",
            "25000/25000 [==============================] - 17s 663us/step - loss: 0.4087 - acc: 0.8815 - val_loss: 0.4323 - val_acc: 0.8654\n",
            "Epoch 6/10\n",
            "25000/25000 [==============================] - 17s 665us/step - loss: 0.3962 - acc: 0.8851 - val_loss: 0.4231 - val_acc: 0.8694\n",
            "Epoch 7/10\n",
            "25000/25000 [==============================] - 16s 658us/step - loss: 0.3951 - acc: 0.8830 - val_loss: 0.4158 - val_acc: 0.8708\n",
            "Epoch 8/10\n",
            "25000/25000 [==============================] - 16s 656us/step - loss: 0.3883 - acc: 0.8850 - val_loss: 0.4503 - val_acc: 0.8533\n",
            "Epoch 9/10\n",
            "25000/25000 [==============================] - 17s 662us/step - loss: 0.3833 - acc: 0.8882 - val_loss: 0.4132 - val_acc: 0.8700\n",
            "Epoch 10/10\n",
            "25000/25000 [==============================] - 16s 657us/step - loss: 0.3796 - acc: 0.8874 - val_loss: 0.4253 - val_acc: 0.8655\n",
            "Accuracy (epoch 7): 0.8830 train, 0.8708 val\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hUfCSWU3-9Rh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## GloVe-based Model (let's start with averaging)"
      ]
    },
    {
      "metadata": {
        "id": "AwWSvDJK-9Rj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Download and extract the official pre-trained GloVe matrix:"
      ]
    },
    {
      "metadata": {
        "id": "oh_CHorSBRjN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "fe190bff-494f-4041-dd19-8914c802c0d8"
      },
      "cell_type": "code",
      "source": [
        "! wget https://nlp.stanford.edu/data/glove.6B.zip\n",
        "! unzip glove.6B.zip\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-02-21 23:03:50--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.56MB/s    in 3m 26s  \n",
            "\n",
            "2019-02-21 23:07:16 (4.00 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "T_VJISCtBX5p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We decided to use GloVe over word2vec as in our experience, GloVe carries word semantics better and works better in semantic-sensitive tasks.  But the difference is not huge."
      ]
    },
    {
      "metadata": {
        "id": "fHrfYbfV-9Rq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Loading GloVe vocabulary"
      ]
    },
    {
      "metadata": {
        "id": "CgWxaO8I-9Rs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You may need to restart your kernel here if you don't have a lot of free memory, to get rid of the bag-of-words vectors from before."
      ]
    },
    {
      "metadata": {
        "id": "FzCS4vLE-9Ru",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Embedding dimension means how many elements each word vector has.\n",
        "# Common values are 50 and 300.  Generally, larger vectors have more\n",
        "# capacity to carry information (meaning).  But the training process\n",
        "# is slower, both vectors and further layers take more memory, and\n",
        "# there could be higher risk of overfitting.\n",
        "EMBEDDING_DIM = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1pH6j5q5-9Ry",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5058940e-f81d-4af4-df5e-4b0e6a912f4f"
      },
      "cell_type": "code",
      "source": [
        "# let's create a dictionary of each word in the pre-trained GloVe embeddings, saving its location indexes \n",
        "import os\n",
        "from tqdm import tqdm\n",
        "GLOVE_DIR = \".\"\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join(GLOVE_DIR, 'glove.6B.%dd.txt' % EMBEDDING_DIM))\n",
        "for line in tqdm(f):\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "400000it [00:05, 78652.60it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "Kj-muSQo-9R4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# a matrix that is indexed by our vocabulary, containing\n",
        "# GloVe embedding for each vocabulary element\n",
        "embedding_matrix = np.zeros((len(vocab) + 1, EMBEDDING_DIM))\n",
        "for i, word in enumerate(vocab):\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        # also, [0] is reserved for padding\n",
        "        embedding_matrix[i + 1] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zTqCYB5n-9R7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Checking how many words have no pre-trained GloVe word embeddings:"
      ]
    },
    {
      "metadata": {
        "id": "h5zua3DU-9R8",
        "colab_type": "code",
        "colab": {},
        "outputId": "2ebd66c5-1bd6-4600-a04e-7c9eddca470a"
      },
      "cell_type": "code",
      "source": [
        "1. * np.count_nonzero(np.all(embedding_matrix == 0, axis=1)) / len(vocab)  # OOV portion"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0076"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "MtQ5rl0R-9SD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We are representing reviews as sequences now.  However, each sequence must have the same length on the input in Keras.  We will pad shorter sequences by zeroes from right, but what should be the maximum sequence length?  Let's find a compromise."
      ]
    },
    {
      "metadata": {
        "id": "PgQLKWwJ-9SF",
        "colab_type": "code",
        "colab": {},
        "outputId": "79a55060-a037-4ee2-d635-7f1a0e729ed1"
      },
      "cell_type": "code",
      "source": [
        "lengths = sorted([len(X) for X in X_train])\n",
        "percentile = 0.90\n",
        "seq_cutoff = lengths[int(len(lengths)*percentile)]\n",
        "print('Longest: %d, Average: %f, Median: %d, %d%% percentile: %d tokens' % (lengths[-1], np.mean(lengths), lengths[int(len(lengths)*0.5)], percentile*100, seq_cutoff))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Longest: 2470, Average: 233.778560, Median: 174, 90% percentile: 458 tokens\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YTymg3Ex-9SP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def vocab_indices_vector(tokens):\n",
        "    vector = [0] * seq_cutoff\n",
        "    if len(tokens) > seq_cutoff:\n",
        "        # Remove the middle\n",
        "        tokens = tokens[: seq_cutoff // 2] + ['SINGLE_PADDING_IN_THE_MIDDLE'] + tokens[-seq_cutoff // 2 :]\n",
        "    for i, t in enumerate(tokens):\n",
        "        try:\n",
        "            vector[i] = vocab.index(t) + 1  # reserving 0 for padding\n",
        "        except:\n",
        "            pass  # ignore missing words\n",
        "    return vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VhwByKqy-9SU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_indices_train = [vocab_indices_vector(x) for x in X_train]\n",
        "X_indices_val = [vocab_indices_vector(x) for x in X_val]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f56NxPuV-9SZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### GloVe averaging model"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "SdZZveHW-9Sa",
        "colab_type": "code",
        "colab": {},
        "outputId": "664f532d-f522-4fb7-abd7-5e30fc6b2321"
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Activation, GlobalAveragePooling1D, Dense, Embedding, Input\n",
        "from keras.models import Model\n",
        "import numpy as np\n",
        "\n",
        "class GloveAvgSentimentModel(object):\n",
        "    def __init__(self):\n",
        "        self.model = self.create()\n",
        "        self.model.summary()\n",
        "        self.model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "        \n",
        "    def create(self):\n",
        "        seq_indices = Input(shape=(seq_cutoff,), name='seq_input')                    \n",
        "        seq_embedded = Embedding(input_dim=len(vocab) + 1, output_dim=EMBEDDING_DIM, weights=[embedding_matrix],\n",
        "                                 input_length=seq_cutoff, trainable=False)(seq_indices)\n",
        "        avg_embedded = GlobalAveragePooling1D()(seq_embedded)\n",
        "        sentiment = Dense(1, activation='sigmoid')(avg_embedded)\n",
        "\n",
        "        return Model(inputs=[seq_indices], outputs=[sentiment])\n",
        "\n",
        "    def train(self, X, y, X_val, y_val):\n",
        "        print('Fitting...')\n",
        "        return self.model.fit(np.array(X, dtype='int32'), y,\n",
        "                              validation_data=(np.array(X_val, dtype='int32'), y_val), epochs=10, verbose=1)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.model.predict(np.array(X))\n",
        "    \n",
        "sentiment = GloveAvgSentimentModel()\n",
        "history = sentiment.train(X_indices_train, y_train, X_indices_val, y_val)\n",
        "best_train_history(history)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "seq_input (InputLayer)       (None, 458)               0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 458, 50)           250050    \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_1 ( (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 250,101\n",
            "Trainable params: 51\n",
            "Non-trainable params: 250,050\n",
            "_________________________________________________________________\n",
            "Fitting...\n",
            "['seq_input']\n",
            "['dense_6']\n",
            "['seq_input']\n",
            "['dense_6']\n",
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/10\n",
            "25000/25000 [==============================] - 3s 133us/step - loss: 0.6879 - acc: 0.5726 - val_loss: 0.6785 - val_acc: 0.6280\n",
            "Epoch 2/10\n",
            "25000/25000 [==============================] - 3s 125us/step - loss: 0.6704 - acc: 0.6473 - val_loss: 0.6655 - val_acc: 0.6472\n",
            "Epoch 3/10\n",
            "25000/25000 [==============================] - 3s 125us/step - loss: 0.6581 - acc: 0.6589 - val_loss: 0.6551 - val_acc: 0.6557\n",
            "Epoch 4/10\n",
            "25000/25000 [==============================] - 3s 123us/step - loss: 0.6482 - acc: 0.6655 - val_loss: 0.6470 - val_acc: 0.6612\n",
            "Epoch 5/10\n",
            "25000/25000 [==============================] - 3s 124us/step - loss: 0.6402 - acc: 0.6713 - val_loss: 0.6401 - val_acc: 0.6670\n",
            "Epoch 6/10\n",
            "25000/25000 [==============================] - 3s 125us/step - loss: 0.6335 - acc: 0.6774 - val_loss: 0.6342 - val_acc: 0.6721\n",
            "Epoch 7/10\n",
            "25000/25000 [==============================] - 3s 124us/step - loss: 0.6276 - acc: 0.6819 - val_loss: 0.6291 - val_acc: 0.6760\n",
            "Epoch 8/10\n",
            "25000/25000 [==============================] - 3s 124us/step - loss: 0.6225 - acc: 0.6856 - val_loss: 0.6245 - val_acc: 0.6800\n",
            "Epoch 9/10\n",
            "25000/25000 [==============================] - 3s 125us/step - loss: 0.6178 - acc: 0.6881 - val_loss: 0.6203 - val_acc: 0.6847\n",
            "Epoch 10/10\n",
            "25000/25000 [==============================] - 3s 123us/step - loss: 0.6137 - acc: 0.6929 - val_loss: 0.6165 - val_acc: 0.6879\n",
            "Accuracy (epoch 10): 0.6929 train, 0.6879 val\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9-Brswov-9Sh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Questions:\n",
        "  * Predict sentiment from mean embedding, or mean sentiment from each embedding?\n",
        "  * Projection to a latent \"sentiment predictive\" vector space first?\n",
        "  \n",
        "Other ideas (for pretty much all further models):\n",
        "  * Trainable embeddings?\n",
        "  * ...what about embeddings from scratch, by the way?\n",
        "  * Just more epochs?\n",
        "\n",
        "Advanced idea:\n",
        "  * Could we visualize importance of individual words towards the predicted sentiment?"
      ]
    },
    {
      "metadata": {
        "id": "YP67FQAl-9Sh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Maximum in a \"sentiment predictive\" space"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "kReH2-iD-9Si",
        "colab_type": "code",
        "colab": {},
        "outputId": "0cecc686-2d5b-44ff-df32-97e0982f1925"
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Activation, GlobalMaxPooling1D, Dense, Embedding, Input\n",
        "from keras.models import Model\n",
        "import numpy as np\n",
        "\n",
        "class GloveHiddenMaxSentimentModel(object):\n",
        "    def __init__(self, N=64):\n",
        "        self.model = self.create(N)\n",
        "        self.model.summary()\n",
        "        self.model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "        \n",
        "    def create(self, N):\n",
        "        seq_indices = Input(shape=(seq_cutoff,), name='seq_input')                    \n",
        "        seq_embedded = Embedding(input_dim=len(vocab) + 1, output_dim=EMBEDDING_DIM, weights=[embedding_matrix],\n",
        "                                 input_length=seq_cutoff, trainable=False)(seq_indices)\n",
        "        seq_hidden = Dense(N, activation='tanh')(seq_embedded)\n",
        "        max_hidden = GlobalMaxPooling1D()(seq_hidden)\n",
        "        sentiment = Dense(1, activation='sigmoid')(max_hidden)\n",
        "\n",
        "        return Model(inputs=[seq_indices], outputs=[sentiment])\n",
        "\n",
        "    def train(self, X, y, X_val, y_val):\n",
        "        print('Fitting...')\n",
        "        return self.model.fit(np.array(X, dtype='int32'), y,\n",
        "                              validation_data=(np.array(X_val, dtype='int32'), y_val), epochs=10, verbose=1)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.model.predict(np.array(X))\n",
        "    \n",
        "sentiment = GloveHiddenMaxSentimentModel()\n",
        "history = sentiment.train(X_indices_train, y_train, X_indices_val, y_val)\n",
        "best_train_history(history)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "seq_input (InputLayer)       (None, 458)               0         \n",
            "_________________________________________________________________\n",
            "embedding_2 (Embedding)      (None, 458, 50)           250050    \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 458, 64)           3264      \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_1 (Glob (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 253,379\n",
            "Trainable params: 3,329\n",
            "Non-trainable params: 250,050\n",
            "_________________________________________________________________\n",
            "Fitting...\n",
            "['seq_input']\n",
            "['dense_8']\n",
            "['seq_input']\n",
            "['dense_8']\n",
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/10\n",
            "25000/25000 [==============================] - 4s 155us/step - loss: 0.5546 - acc: 0.7292 - val_loss: 0.4599 - val_acc: 0.7936\n",
            "Epoch 2/10\n",
            "25000/25000 [==============================] - 4s 144us/step - loss: 0.4316 - acc: 0.8094 - val_loss: 0.4171 - val_acc: 0.8119\n",
            "Epoch 3/10\n",
            "25000/25000 [==============================] - 4s 145us/step - loss: 0.3946 - acc: 0.8284 - val_loss: 0.4106 - val_acc: 0.8125\n",
            "Epoch 4/10\n",
            "25000/25000 [==============================] - 4s 147us/step - loss: 0.3717 - acc: 0.8409 - val_loss: 0.3843 - val_acc: 0.8272\n",
            "Epoch 5/10\n",
            "25000/25000 [==============================] - 4s 146us/step - loss: 0.3566 - acc: 0.8474 - val_loss: 0.3786 - val_acc: 0.8289\n",
            "Epoch 6/10\n",
            "25000/25000 [==============================] - 4s 147us/step - loss: 0.3432 - acc: 0.8526 - val_loss: 0.3736 - val_acc: 0.8318\n",
            "Epoch 7/10\n",
            "25000/25000 [==============================] - 4s 147us/step - loss: 0.3328 - acc: 0.8572 - val_loss: 0.3817 - val_acc: 0.8264\n",
            "Epoch 8/10\n",
            "25000/25000 [==============================] - 4s 145us/step - loss: 0.3251 - acc: 0.8625 - val_loss: 0.3688 - val_acc: 0.8352\n",
            "Epoch 9/10\n",
            "25000/25000 [==============================] - 4s 144us/step - loss: 0.3184 - acc: 0.8652 - val_loss: 0.3665 - val_acc: 0.8362\n",
            "Epoch 10/10\n",
            "25000/25000 [==============================] - 4s 145us/step - loss: 0.3121 - acc: 0.8683 - val_loss: 0.3675 - val_acc: 0.8357\n",
            "Accuracy (epoch 9): 0.8652 train, 0.8362 val\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mSIrpXsu-9Sm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Ideas:\n",
        "  * Averaging rather than maximum?\n",
        "  * Do other N values make a difference?"
      ]
    },
    {
      "metadata": {
        "id": "l32vkRE8-9Sn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Convolutions on sequences of embeddings"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "mTTc1kwo-9So",
        "colab_type": "code",
        "colab": {},
        "outputId": "699cb6a6-6a28-437c-8d22-8d3880a0052e"
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Activation, Conv1D, Dense, Embedding, GlobalMaxPooling1D, Input\n",
        "from keras.models import Model\n",
        "import numpy as np\n",
        "\n",
        "class GloveCNNSentimentModel(object):\n",
        "    def __init__(self, N=64, size=3):\n",
        "        self.model = self.create(N, size)\n",
        "        self.model.summary()\n",
        "        self.model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "        \n",
        "    def create(self, N, size):\n",
        "        seq_indices = Input(shape=(seq_cutoff,), name='seq_input') \n",
        "        seq_embedded = Embedding(input_dim=len(vocab) + 1, output_dim=EMBEDDING_DIM, weights=[embedding_matrix],\n",
        "                                 input_length=seq_cutoff, trainable=False)(seq_indices)\n",
        "        seq_conv = Conv1D(N, size, activation='tanh')(seq_embedded)\n",
        "        max_conv = GlobalMaxPooling1D()(seq_conv)\n",
        "        hidden_repr = Dense(N, activation='tanh')(max_conv)\n",
        "        sentiment = Dense(1, activation='sigmoid')(hidden_repr)\n",
        "\n",
        "        return Model(inputs=[seq_indices], outputs=[sentiment])\n",
        "\n",
        "    def train(self, X, y, X_val, y_val):\n",
        "        print('Fitting...')\n",
        "        return self.model.fit(np.array(X, dtype='int32'), y,\n",
        "                              validation_data=(np.array(X_val, dtype='int32'), y_val), epochs=10, verbose=1)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.model.predict(np.array(X))\n",
        "    \n",
        "sentiment = GloveCNNSentimentModel()\n",
        "history = sentiment.train(X_indices_train, y_train, X_indices_val, y_val)\n",
        "best_train_history(history)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "seq_input (InputLayer)       (None, 458)               0         \n",
            "_________________________________________________________________\n",
            "embedding_3 (Embedding)      (None, 458, 50)           250050    \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 456, 64)           9664      \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_2 (Glob (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 263,939\n",
            "Trainable params: 13,889\n",
            "Non-trainable params: 250,050\n",
            "_________________________________________________________________\n",
            "Fitting...\n",
            "['seq_input']\n",
            "['dense_10']\n",
            "['seq_input']\n",
            "['dense_10']\n",
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/10\n",
            "25000/25000 [==============================] - 6s 240us/step - loss: 0.4946 - acc: 0.7489 - val_loss: 0.4146 - val_acc: 0.8111\n",
            "Epoch 2/10\n",
            "25000/25000 [==============================] - 4s 168us/step - loss: 0.3692 - acc: 0.8385 - val_loss: 0.3745 - val_acc: 0.8340\n",
            "Epoch 3/10\n",
            "25000/25000 [==============================] - 4s 167us/step - loss: 0.3295 - acc: 0.8583 - val_loss: 0.4009 - val_acc: 0.8249\n",
            "Epoch 4/10\n",
            "25000/25000 [==============================] - 4s 167us/step - loss: 0.3007 - acc: 0.8734 - val_loss: 0.3925 - val_acc: 0.8296\n",
            "Epoch 5/10\n",
            "25000/25000 [==============================] - 4s 167us/step - loss: 0.2730 - acc: 0.8872 - val_loss: 0.3785 - val_acc: 0.8374\n",
            "Epoch 6/10\n",
            "25000/25000 [==============================] - 4s 168us/step - loss: 0.2498 - acc: 0.8979 - val_loss: 0.3955 - val_acc: 0.8358\n",
            "Epoch 7/10\n",
            "25000/25000 [==============================] - 4s 167us/step - loss: 0.2358 - acc: 0.9030 - val_loss: 0.4073 - val_acc: 0.8319\n",
            "Epoch 8/10\n",
            "25000/25000 [==============================] - 4s 168us/step - loss: 0.2138 - acc: 0.9129 - val_loss: 0.5104 - val_acc: 0.8086\n",
            "Epoch 9/10\n",
            "25000/25000 [==============================] - 4s 166us/step - loss: 0.2093 - acc: 0.9157 - val_loss: 0.4437 - val_acc: 0.8274\n",
            "Epoch 10/10\n",
            "25000/25000 [==============================] - 4s 168us/step - loss: 0.1935 - acc: 0.9227 - val_loss: 0.4514 - val_acc: 0.8310\n",
            "Accuracy (epoch 5): 0.8872 train, 0.8374 val\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YwdxR4-K-9Su",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This model still sucks.  What problems does it have?\n",
        "\n",
        "But the model below is awesome.  (Stolen from Keras' `examples/imdb_cnn.py`.)\n",
        "  * What are the differences to the model above?\n",
        "  * What exactly makes it so awesome?\n",
        "  * Did we beat the bag-of-words baseline yet?"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "3J5yeVx5-9Sv",
        "colab_type": "code",
        "colab": {},
        "outputId": "2a2c7a86-396a-4a93-8959-e91cf3a5da60"
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Activation, Conv1D, Dense, Embedding, GlobalMaxPooling1D, Input\n",
        "from keras.models import Model\n",
        "import numpy as np\n",
        "\n",
        "class GloveCNNAwesomeSentimentModel(object):\n",
        "    def __init__(self, N=256, size=3):\n",
        "        self.model = self.create(N, size)\n",
        "        self.model.summary()\n",
        "        self.model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "        \n",
        "    def create(self, N, size):\n",
        "        seq_indices = Input(shape=(seq_cutoff,), name='seq_input') \n",
        "        seq_embedded = Embedding(input_dim=len(vocab) + 1, output_dim=EMBEDDING_DIM,\n",
        "                                 input_length=seq_cutoff)(seq_indices)\n",
        "        seq_conv = Conv1D(N, size, activation='relu')(Dropout(0.2)(seq_embedded))\n",
        "        max_conv = GlobalMaxPooling1D()(seq_conv)\n",
        "        hidden_repr = Dense(N, activation='relu')(max_conv)\n",
        "        sentiment = Dense(1, activation='sigmoid')(Dropout(0.2)(hidden_repr))\n",
        "\n",
        "        return Model(inputs=[seq_indices], outputs=[sentiment])\n",
        "\n",
        "    def train(self, X, y, X_val, y_val):\n",
        "        print('Fitting...')\n",
        "        return self.model.fit(np.array(X, dtype='int32'), y,\n",
        "                              validation_data=(np.array(X_val, dtype='int32'), y_val), epochs=10, verbose=1)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.model.predict(np.array(X))\n",
        "    \n",
        "sentiment = GloveCNNAwesomeSentimentModel()\n",
        "history = sentiment.train(X_indices_train, y_train, X_indices_val, y_val)\n",
        "best_train_history(history)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "seq_input (InputLayer)       (None, 458)               0         \n",
            "_________________________________________________________________\n",
            "embedding_4 (Embedding)      (None, 458, 50)           250050    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 458, 50)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 456, 256)          38656     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_3 (Glob (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 354,755\n",
            "Trainable params: 354,755\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Fitting...\n",
            "['seq_input']\n",
            "['dense_12']\n",
            "['seq_input']\n",
            "['dense_12']\n",
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/10\n",
            "25000/25000 [==============================] - 6s 241us/step - loss: 0.4191 - acc: 0.7960 - val_loss: 0.2955 - val_acc: 0.8770\n",
            "Epoch 2/10\n",
            "25000/25000 [==============================] - 6s 224us/step - loss: 0.2512 - acc: 0.8972 - val_loss: 0.2800 - val_acc: 0.8869\n",
            "Epoch 3/10\n",
            "25000/25000 [==============================] - 6s 222us/step - loss: 0.1829 - acc: 0.9278 - val_loss: 0.3035 - val_acc: 0.8794\n",
            "Epoch 4/10\n",
            "25000/25000 [==============================] - 6s 223us/step - loss: 0.1251 - acc: 0.9532 - val_loss: 0.3131 - val_acc: 0.8812\n",
            "Epoch 5/10\n",
            "25000/25000 [==============================] - 6s 225us/step - loss: 0.0889 - acc: 0.9672 - val_loss: 0.3449 - val_acc: 0.8795\n",
            "Epoch 6/10\n",
            "25000/25000 [==============================] - 6s 222us/step - loss: 0.0610 - acc: 0.9780 - val_loss: 0.4176 - val_acc: 0.8774\n",
            "Epoch 7/10\n",
            "25000/25000 [==============================] - 6s 221us/step - loss: 0.0499 - acc: 0.9812 - val_loss: 0.5348 - val_acc: 0.8638\n",
            "Epoch 8/10\n",
            "25000/25000 [==============================] - 6s 226us/step - loss: 0.0415 - acc: 0.9850 - val_loss: 0.4720 - val_acc: 0.8758\n",
            "Epoch 9/10\n",
            "25000/25000 [==============================] - 6s 224us/step - loss: 0.0351 - acc: 0.9878 - val_loss: 0.5985 - val_acc: 0.8618\n",
            "Epoch 10/10\n",
            "25000/25000 [==============================] - 6s 224us/step - loss: 0.0320 - acc: 0.9885 - val_loss: 0.5249 - val_acc: 0.8746\n",
            "Accuracy (epoch 2): 0.8972 train, 0.8869 val\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sQOd6dhh-9S8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Can we make the model more awesome yet?\n",
        "\n",
        "Ideas:\n",
        "  * So what about the Glove pretrained embeddings?\n",
        "  * Project the embeddings to a more convolution-friendly space?\n",
        "  * Average pooling instead of max?\n",
        "  * Is 300D embedding better?\n",
        "  \n",
        "Advanced ideas:\n",
        "  * What about _both_ average and max pooling?\n",
        "  * `relu` activation is popular in computer vision CNNs instead of `tanh`.\n",
        "  * Some people recommend `rmsprop` optimizer rather than `adam` for CNNs. Does it matter?"
      ]
    },
    {
      "metadata": {
        "id": "fcuz_-sZ-9S9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Recurrence on sequences of embeddings"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "BWQFFtaz-9S_",
        "colab_type": "code",
        "colab": {},
        "outputId": "c95af15b-a8fe-4358-ec29-26db406b69f5"
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Activation, Dense, Embedding, GRU, Input\n",
        "from keras.models import Model\n",
        "import numpy as np\n",
        "\n",
        "class GloveGRUSentimentModel(object):\n",
        "    def __init__(self, N=64):\n",
        "        self.model = self.create(N)\n",
        "        self.model.summary()\n",
        "        self.model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "        \n",
        "    def create(self, N):\n",
        "        seq_indices = Input(shape=(seq_cutoff,), name='seq_input') \n",
        "        seq_embedded = Embedding(input_dim=len(vocab) + 1, output_dim=EMBEDDING_DIM, weights=[embedding_matrix],\n",
        "                                 input_length=seq_cutoff, trainable=False)(seq_indices)\n",
        "        recursive_repr = GRU(N)(seq_embedded)\n",
        "        sentiment = Dense(1, activation='sigmoid')(recursive_repr)\n",
        "\n",
        "        return Model(inputs=[seq_indices], outputs=[sentiment])\n",
        "\n",
        "    def train(self, X, y, X_val, y_val):\n",
        "        print('Fitting...')\n",
        "        return self.model.fit(np.array(X, dtype='int32'), y,\n",
        "                              validation_data=(np.array(X_val, dtype='int32'), y_val), epochs=10, verbose=1)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.model.predict(np.array(X))\n",
        "    \n",
        "sentiment = GloveGRUSentimentModel()\n",
        "history = sentiment.train(X_indices_train, y_train, X_indices_val, y_val)\n",
        "best_train_history(history)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "seq_input (InputLayer)       (None, 458)               0         \n",
            "_________________________________________________________________\n",
            "embedding_5 (Embedding)      (None, 458, 50)           250050    \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (None, 64)                22080     \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 272,195\n",
            "Trainable params: 22,145\n",
            "Non-trainable params: 250,050\n",
            "_________________________________________________________________\n",
            "Fitting...\n",
            "['seq_input']\n",
            "['dense_13']\n",
            "['seq_input']\n",
            "['dense_13']\n",
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/10\n",
            "25000/25000 [==============================] - 540s 22ms/step - loss: 0.6910 - acc: 0.5092 - val_loss: 0.6826 - val_acc: 0.5236\n",
            "Epoch 2/10\n",
            "25000/25000 [==============================] - 536s 21ms/step - loss: 0.5446 - acc: 0.6851 - val_loss: 0.4029 - val_acc: 0.8170\n",
            "Epoch 3/10\n",
            "25000/25000 [==============================] - 527s 21ms/step - loss: 0.3816 - acc: 0.8287 - val_loss: 0.3633 - val_acc: 0.8353\n",
            "Epoch 4/10\n",
            "25000/25000 [==============================] - 527s 21ms/step - loss: 0.3472 - acc: 0.8488 - val_loss: 0.3377 - val_acc: 0.8502\n",
            "Epoch 5/10\n",
            "25000/25000 [==============================] - 518s 21ms/step - loss: 0.3239 - acc: 0.8586 - val_loss: 0.3275 - val_acc: 0.8560\n",
            "Epoch 6/10\n",
            "25000/25000 [==============================] - 523s 21ms/step - loss: 0.3009 - acc: 0.8702 - val_loss: 0.3432 - val_acc: 0.8504\n",
            "Epoch 7/10\n",
            "25000/25000 [==============================] - 526s 21ms/step - loss: 0.2890 - acc: 0.8768 - val_loss: 0.3201 - val_acc: 0.8607\n",
            "Epoch 8/10\n",
            "25000/25000 [==============================] - 520s 21ms/step - loss: 0.2680 - acc: 0.8852 - val_loss: 0.3279 - val_acc: 0.8537\n",
            "Epoch 9/10\n",
            "25000/25000 [==============================] - 524s 21ms/step - loss: 0.2549 - acc: 0.8939 - val_loss: 0.3133 - val_acc: 0.8665\n",
            "Epoch 10/10\n",
            "25000/25000 [==============================] - 529s 21ms/step - loss: 0.2428 - acc: 0.9001 - val_loss: 0.3242 - val_acc: 0.8631\n",
            "Accuracy (epoch 9): 0.8939 train, 0.8665 val\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "utKczfdI-9TE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If you take the time to train the model, it seems to really suck after the first epoch.  What could be the problem?  (Imagine the input, and how the RNN processes it.)"
      ]
    },
    {
      "metadata": {
        "id": "9ey9CBLy-9TF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The model below fixes the problem - how?  What is the most essential change?"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "ObCeEzVF-9TI",
        "colab_type": "code",
        "colab": {},
        "outputId": "e1c44011-0fbf-4c10-e13c-3a18e55a5516"
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Activation, Bidirectional, Dense, Embedding, GRU, Input\n",
        "from keras.models import Model\n",
        "import numpy as np\n",
        "\n",
        "class GloveGRUSentimentModel(object):\n",
        "    def __init__(self, N=64):\n",
        "        self.model = self.create(N)\n",
        "        self.model.summary()\n",
        "        self.model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "        \n",
        "    def create(self, N):\n",
        "        seq_indices = Input(shape=(seq_cutoff,), name='seq_input') \n",
        "        seq_embedded = Embedding(input_dim=len(vocab) + 1, output_dim=EMBEDDING_DIM, weights=[embedding_matrix],\n",
        "                                 input_length=seq_cutoff, mask_zero=True)(seq_indices)\n",
        "        recursive_repr = Bidirectional(GRU(N))(seq_embedded)\n",
        "        sentiment = Dense(1, activation='sigmoid')(recursive_repr)\n",
        "\n",
        "        return Model(inputs=[seq_indices], outputs=[sentiment])\n",
        "\n",
        "    def train(self, X, y, X_val, y_val):\n",
        "        print('Fitting...')\n",
        "        return self.model.fit(np.array(X, dtype='int32'), y,\n",
        "                              validation_data=(np.array(X_val, dtype='int32'), y_val), epochs=10, verbose=1,\n",
        "                             batch_size=128)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.model.predict(np.array(X))\n",
        "    \n",
        "sentiment = GloveGRUSentimentModel()\n",
        "history = sentiment.train(X_indices_train, y_train, X_indices_val, y_val)\n",
        "best_train_history(history)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "seq_input (InputLayer)       (None, 458)               0         \n",
            "_________________________________________________________________\n",
            "embedding_6 (Embedding)      (None, 458, 50)           250050    \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 128)               44160     \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 294,339\n",
            "Trainable params: 294,339\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Fitting...\n",
            "['seq_input']\n",
            "['dense_14']\n",
            "['seq_input']\n",
            "['dense_14']\n",
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/10\n",
            "25000/25000 [==============================] - 306s 12ms/step - loss: 0.5319 - acc: 0.7134 - val_loss: 0.3534 - val_acc: 0.8473\n",
            "Epoch 2/10\n",
            "25000/25000 [==============================] - 307s 12ms/step - loss: 0.3012 - acc: 0.8737 - val_loss: 0.2861 - val_acc: 0.8796\n",
            "Epoch 3/10\n",
            "25000/25000 [==============================] - 307s 12ms/step - loss: 0.2445 - acc: 0.9011 - val_loss: 0.3012 - val_acc: 0.8715\n",
            "Epoch 4/10\n",
            "25000/25000 [==============================] - 303s 12ms/step - loss: 0.2094 - acc: 0.9182 - val_loss: 0.2831 - val_acc: 0.8879\n",
            "Epoch 5/10\n",
            "25000/25000 [==============================] - 303s 12ms/step - loss: 0.1752 - acc: 0.9330 - val_loss: 0.3280 - val_acc: 0.8724\n",
            "Epoch 6/10\n",
            "25000/25000 [==============================] - 307s 12ms/step - loss: 0.1453 - acc: 0.9461 - val_loss: 0.3160 - val_acc: 0.8874\n",
            "Epoch 7/10\n",
            "25000/25000 [==============================] - 308s 12ms/step - loss: 0.1195 - acc: 0.9563 - val_loss: 0.3404 - val_acc: 0.8826\n",
            "Epoch 8/10\n",
            "25000/25000 [==============================] - 306s 12ms/step - loss: 0.0907 - acc: 0.9699 - val_loss: 0.3737 - val_acc: 0.8775\n",
            "Epoch 9/10\n",
            "25000/25000 [==============================] - 305s 12ms/step - loss: 0.0631 - acc: 0.9806 - val_loss: 0.4440 - val_acc: 0.8740\n",
            "Epoch 10/10\n",
            "25000/25000 [==============================] - 304s 12ms/step - loss: 0.0398 - acc: 0.9894 - val_loss: 0.5192 - val_acc: 0.8696\n",
            "Accuracy (epoch 4): 0.9182 train, 0.8879 val\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TCXUIMyN-9TO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We are overfitting.  Can we improve generalization?\n",
        "\n",
        "Ideas:\n",
        "  * `GRU(N, return_sequence=True)` would return a single output element for each input element, rather than a single output for the whole sequence.  What cool things could we use this for?\n",
        "  * Any ideas to borrow from CNNs?\n",
        "  * Could we stack multiple GRUs on top of each other?"
      ]
    },
    {
      "metadata": {
        "id": "y6PUAKrA-9TR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Combining CNN and GRU"
      ]
    },
    {
      "metadata": {
        "id": "YsXZBfAa-9TT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's go wild and take a look at an even more advanced model combining several ideas.  Can you explain what's going on here? Is this madness worth it? Which further improvements might be most desirable to test?"
      ]
    },
    {
      "metadata": {
        "id": "HP3eGl6d-9TU",
        "colab_type": "code",
        "colab": {},
        "outputId": "c67afede-216a-46a3-8cae-48cbb5aab0f9"
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Activation, Bidirectional, Conv1D, Dense, Embedding, GlobalMaxPooling1D, GRU, Input, add\n",
        "from keras.models import Model\n",
        "import numpy as np\n",
        "\n",
        "class GloveCNNGRUSISentimentModel(object):\n",
        "    def __init__(self, N=64, size=3):\n",
        "        self.model = self.create(N, size)\n",
        "        self.model.summary()\n",
        "        self.model.compile(optimizer='adam', loss='binary_crossentropy', loss_weights=[0.2, 1.0], metrics=['accuracy'])\n",
        "        \n",
        "    def create(self, N, size):\n",
        "        seq_indices = Input(shape=(seq_cutoff,), name='seq_input') \n",
        "        seq_embedded = Embedding(input_dim=len(vocab) + 1, output_dim=EMBEDDING_DIM, weights=[embedding_matrix],\n",
        "                                 input_length=seq_cutoff)(seq_indices)\n",
        "        seq_hidden = Dense(N, activation='tanh')(Dropout(0.2)(seq_embedded))\n",
        "        \n",
        "        seq_conv = Conv1D(N, size, activation='tanh', padding='same')(Dropout(0.2)(seq_hidden))\n",
        "        # Residual skip-connection: The convolution just fine-tunes per-word Dense projection.\n",
        "        seq_hidden = add([seq_hidden, seq_conv])\n",
        "        \n",
        "        recursive_repr = Bidirectional(GRU(N))(seq_hidden)\n",
        "        sentiment = Dense(1, activation='sigmoid', name='sentiment')(recursive_repr)\n",
        "        \n",
        "        # Inception: Also try to make even the hidden representation good enough to predict sentiment.\n",
        "        max_hidden = GlobalMaxPooling1D()(seq_hidden)\n",
        "        sentiment_by_hidden = Dense(1, activation='sigmoid', name='sentiment_by_hidden')(max_hidden)\n",
        "\n",
        "        return Model(inputs=[seq_indices], outputs=[sentiment_by_hidden, sentiment])\n",
        "\n",
        "    def train(self, X, y, X_val, y_val):\n",
        "        print('Fitting...')\n",
        "        return self.model.fit(np.array(X, dtype='int32'), [np.array(y), np.array(y)],\n",
        "                              validation_data=(np.array(X_val, dtype='int32'),\n",
        "                                               [np.array(y_val), np.array(y_val)]),\n",
        "                              epochs=10, verbose=1)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.model.predict(np.array(X))[-1:]\n",
        "    \n",
        "sentiment = GloveCNNGRUSISentimentModel()\n",
        "history = sentiment.train(X_indices_train, y_train, X_indices_val, y_val)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "seq_input (InputLayer)          (None, 458)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_7 (Embedding)         (None, 458, 50)      250050      seq_input[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 458, 50)      0           embedding_7[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_15 (Dense)                (None, 458, 64)      3264        dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 458, 64)      0           dense_15[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 458, 64)      12352       dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 458, 64)      0           dense_15[0][0]                   \n",
            "                                                                 conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_4 (GlobalM (None, 64)           0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_2 (Bidirectional) (None, 128)          49536       add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "sentiment_by_hidden (Dense)     (None, 1)            65          global_max_pooling1d_4[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "sentiment (Dense)               (None, 1)            129         bidirectional_2[0][0]            \n",
            "==================================================================================================\n",
            "Total params: 315,396\n",
            "Trainable params: 315,396\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Fitting...\n",
            "['seq_input']\n",
            "['sentiment_by_hidden', 'sentiment']\n",
            "['seq_input']\n",
            "['sentiment_by_hidden', 'sentiment']\n",
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/10\n",
            "25000/25000 [==============================] - 1047s 42ms/step - loss: 0.5901 - sentiment_by_hidden_loss: 0.5373 - sentiment_loss: 0.4826 - sentiment_by_hidden_acc: 0.7392 - sentiment_acc: 0.7476 - val_loss: 0.4038 - val_sentiment_by_hidden_loss: 0.3823 - val_sentiment_loss: 0.3274 - val_sentiment_by_hidden_acc: 0.8323 - val_sentiment_acc: 0.8634\n",
            "Epoch 2/10\n",
            "25000/25000 [==============================] - 946s 38ms/step - loss: 0.3874 - sentiment_by_hidden_loss: 0.3742 - sentiment_loss: 0.3125 - sentiment_by_hidden_acc: 0.8425 - sentiment_acc: 0.8664 - val_loss: 0.3651 - val_sentiment_by_hidden_loss: 0.3371 - val_sentiment_loss: 0.2976 - val_sentiment_by_hidden_acc: 0.8588 - val_sentiment_acc: 0.8724\n",
            "Epoch 3/10\n",
            "25000/25000 [==============================] - 931s 37ms/step - loss: 0.3332 - sentiment_by_hidden_loss: 0.3287 - sentiment_loss: 0.2675 - sentiment_by_hidden_acc: 0.8644 - sentiment_acc: 0.8889 - val_loss: 0.3490 - val_sentiment_by_hidden_loss: 0.3193 - val_sentiment_loss: 0.2851 - val_sentiment_by_hidden_acc: 0.8656 - val_sentiment_acc: 0.8832\n",
            "Epoch 4/10\n",
            "25000/25000 [==============================] - 948s 38ms/step - loss: 0.3013 - sentiment_by_hidden_loss: 0.3011 - sentiment_loss: 0.2411 - sentiment_by_hidden_acc: 0.8757 - sentiment_acc: 0.9020 - val_loss: 0.3201 - val_sentiment_by_hidden_loss: 0.3130 - val_sentiment_loss: 0.2575 - val_sentiment_by_hidden_acc: 0.8685 - val_sentiment_acc: 0.8935\n",
            "Epoch 5/10\n",
            "25000/25000 [==============================] - 1038s 42ms/step - loss: 0.2738 - sentiment_by_hidden_loss: 0.2822 - sentiment_loss: 0.2173 - sentiment_by_hidden_acc: 0.8866 - sentiment_acc: 0.9147 - val_loss: 0.3215 - val_sentiment_by_hidden_loss: 0.3108 - val_sentiment_loss: 0.2593 - val_sentiment_by_hidden_acc: 0.8705 - val_sentiment_acc: 0.8962\n",
            "Epoch 6/10\n",
            "25000/25000 [==============================] - 1065s 43ms/step - loss: 0.2577 - sentiment_by_hidden_loss: 0.2716 - sentiment_loss: 0.2034 - sentiment_by_hidden_acc: 0.8904 - sentiment_acc: 0.9198 - val_loss: 0.3290 - val_sentiment_by_hidden_loss: 0.3118 - val_sentiment_loss: 0.2666 - val_sentiment_by_hidden_acc: 0.8706 - val_sentiment_acc: 0.8902\n",
            "Epoch 7/10\n",
            "25000/25000 [==============================] - 1038s 42ms/step - loss: 0.2325 - sentiment_by_hidden_loss: 0.2570 - sentiment_loss: 0.1811 - sentiment_by_hidden_acc: 0.8966 - sentiment_acc: 0.9308 - val_loss: 0.3355 - val_sentiment_by_hidden_loss: 0.3179 - val_sentiment_loss: 0.2719 - val_sentiment_by_hidden_acc: 0.8692 - val_sentiment_acc: 0.8940\n",
            "Epoch 8/10\n",
            "25000/25000 [==============================] - 1030s 41ms/step - loss: 0.2211 - sentiment_by_hidden_loss: 0.2480 - sentiment_loss: 0.1715 - sentiment_by_hidden_acc: 0.9011 - sentiment_acc: 0.9332 - val_loss: 0.3501 - val_sentiment_by_hidden_loss: 0.3197 - val_sentiment_loss: 0.2861 - val_sentiment_by_hidden_acc: 0.8723 - val_sentiment_acc: 0.8925\n",
            "Epoch 9/10\n",
            "12224/25000 [=============>................] - ETA: 7:11 - loss: 0.1980 - sentiment_by_hidden_loss: 0.2361 - sentiment_loss: 0.1508 - sentiment_by_hidden_acc: 0.9080 - sentiment_acc: 0.9431"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MBr6-9Z--9Td",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Can we do even better? Yes - let's look at the literature.\n",
        "\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "...and https://arxiv.org/pdf/1605.07725.pdf (source of table above; 5.91%)"
      ]
    },
    {
      "metadata": {
        "id": "5WxfIXq0-9Te",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Final challenge: What common idea is shared by all of the best models? Can you implement one of them based on what you learned today?"
      ]
    },
    {
      "metadata": {
        "id": "OqWwaj-r-9Tf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}